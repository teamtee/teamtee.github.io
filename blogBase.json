{"singlePage": ["about"], "startSite": "", "filingNum": "", "onePageListNum": 15, "commentLabelColor": "#006b75", "yearColorList": ["#bc4c00", "#0969da", "#1f883d", "#A333D0"], "i18n": "CN", "themeMode": "manual", "dayTheme": "light", "nightTheme": "dark_colorblind", "urlMode": "pinyin", "script": "", "style": "", "head": "", "indexScript": "", "indexStyle": "", "bottomText": "\u8f6c\u8f7d\u8bf7\u6ce8\u660e\u51fa\u5904", "showPostSource": 1, "iconList": {"music": "M0 8a8 8 0 1 1 16 0A8 8 0 0 1 0 8Zm8-6.5a6.5 6.5 0 1 0 0 13 6.5 6.5 0 0 0 0-13Z"}, "UTC": 8, "rssSplit": "sentence", "exlink": {}, "needComment": 0, "allHead": "", "title": "teamtee", "subTitle": "\u8fd9\u91cc\u662f\u63d0\u59c6\u63d0\u7684\u65b0\u5c0f\u5c4b\uff0c\u65e7\u5c0f\u5c4b\u53ef\u4ee5\u53bb[\u8fd9\u91cc](http://teamtee.top/teamtee/)", "avatarUrl": "https://github.githubassets.com/favicons/favicon.svg", "GMEEK_VERSION": "last", "displayTitle": "Teamtee", "homeUrl": "http://teamtee.top", "faviconUrl": "https://github.githubassets.com/favicons/favicon.svg", "email": "2624071330@qq.com", "primerCSS": "<link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />", "postListJson": {"P2": {"htmlDir": "docs/post/yi-wen-gao-dong-Whisper-xi-lie-1--shi-yong-Whisper-jin-xing-shi-bie-he-te-zheng-ti-qu.html", "labels": ["QuickNote"], "postTitle": "\u4e00\u6587\u641e\u61c2Whisper\u7cfb\u52171:\u4f7f\u7528Whisper\u8fdb\u884c\u8bc6\u522b\u548c\u7279\u5f81\u63d0\u53d6", "postUrl": "post/yi-wen-gao-dong-Whisper-xi-lie-1--shi-yong-Whisper-jin-xing-shi-bie-he-te-zheng-ti-qu.html", "postSourceUrl": "https://github.com/teamtee/teamtee.github.io/issues/2", "commentNum": 0, "wordCount": 4321, "description": "\r\n\r\n[Whisper\u7684PT\u6587\u4ef6\u4e0b\u8f7d\u5730\u5740](https://gitcode.csdn.net/65ed73ad1a836825ed799909.html)\r\n[\u5728Colab\u5fae\u8c03Whisper](https://huggingface.co/blog/fine-tune-whisper)\r\n\r\n## Whisper\u7b80\u4ecb\r\n\r\nWhisper\u662fOpenai\u5f00\u53d1\u7684\u8bed\u97f3\u8bc6\u522b\u5de5\u5177\uff0c\u901a\u5e38\u6211\u4eec\u53ef\u4ee5\u7528Whisper\u5e93\u6216\u8005Transformers\u6765\u4f7f\u7528Whisper\uff0c\u672c\u6587\u4e13\u6ce8\u4e8eWhisper\u5e93\u7684\u4f7f\u7528\uff0c\u5b89\u88c5\u65b9\u5f0f\u5982\u4e0b\r\n\r\n```python\r\npip install -U openai-whisper\r\n```\r\n\r\n\u8fd8\u9700\u8981\u5b89\u88c5ffmpeg\r\n```\r\nconda install ffmpeg(\u652f\u6301\u975esudo\u7528\u6237)\r\nsudo apt install ffmpeg \r\n```\r\n\r\nWhisper\u5305\u542bencoder\u548cdecoder\u4e24\u4e2a\u90e8\u5206,encoder\u63a5\u53d730s\u7684\u97f3\u9891\u957f\u5ea6\u7684\u8f93\u51fa\uff0c\u7f16\u7801\u6210\u4e3a\u7279\u5f81\u5411\u91cf\uff0cdecoder\u8d1f\u8d23\u89e3\u7801\r\n\r\n\r\n## Whisper\u8bc6\u522b\r\n`transcribe`:\r\n- \u6700\u7b80\u5355\u7684\u8bc6\u522b\u65b9\u5f0f\r\n```python\r\nimport whisper\r\nmodel = whisper.load('path/name')\r\ntext = model.transcribe('wav_path')\r\n\r\n```\r\n`decode`\uff1a\r\n- \u6ce8\u610f\u5230\u97f3\u9891\u4f1a\u88ab`pad_or_trim`\u51fd\u6570\u586b\u5145\u6216\u8005\u88c1\u526a\u4e3a30s\u957f\u5ea6\r\n- `decode`\u652f\u6301`mel`\u8f93\u5165\u6216\u8005`encoder`\u7f16\u7801\u540e\u7684\u7279\u5f81\u8f93\u5165\r\n- `nmels=80/128`,128\u9002\u5408v3\uff0c80\u9002\u5408\u5176\u4ed6\u7248\u672c\r\n```python\r\nimport whisper\r\nimport numpy as np\r\nmodel = whisper.load_model('')\r\naudio = whisper.load_audio('')\r\naudio = whisper.pad_or_trim(audio)\r\nmel = whisper.log_mel_spectrogram(audio,n_mels=model.dims.n_mels).to('cuda').to(model.device)\r\nresult = model.decode(mel)\r\n# result = whisper.decode(model, mel)\r\n\r\n\r\n```\r\n\r\n```python\r\nimport whisper\r\nimport numpy as np\r\nmodel = whisper.load_model('')\r\naudio = whisper.load_audio('')\r\naudio = whisper.pad_or_trim(audio)\r\nmel = whisper.log_mel_spectrogram(audio,n_mels=model.dims.n_mels).to('cuda').to(model.device)\r\nresult =model.decode(mel)\r\nencoder_output = model.encoder(mel.unsqueeze(0))\r\n\r\nresult = model.decode(encoder_output)\r\n# result = whisper.decode(model, encoder_output)\r\n# \u6253\u5370encoder\u8f93\u51fa\u7684\u5f62\u72b6\r\n```\r\n## Whisper\u63d0\u53d6\u7279\u5f81\r\n\u5982\u679c\u91c7\u7528whisper\u7684encoder\u63d0\u53d6\u7279\u5f81\uff0c\u97f3\u9891\u9996\u5148\u8981\u88ab\u586b\u5145\u523030s\r\n```python\r\nimport whisper\r\nimport numpy as np\r\nmodel = whisper.load_model('')\r\naudio = whisper.load_audio('')\r\naudio = whisper.pad_or_trim(audio)\r\nmel = whisper.log_mel_spectrogram(audio,n_mels=model.dims.n_mels).to('cuda').to(model.device)\r\nresult =model.decode(mel)\r\nencoder_output = model.encoder(mel.unsqueeze(0))\r\n\r\n```\r\n\r\n\u53ef\u4ee5\u91c7\u7528\u66ff\u4ee3forward\u51fd\u6570\u7684\u65b9\u6cd5\u6765\u63d0\u53d6\u4e0d\u5b9a\u957f\u5ea6\u7684\u7279\u5f81,\u56e0\u4e3aencoder\u4e0d\u652f\u6301\u5c0f\u4e8e30s\u957f\u5ea6\u97f3\u9891\u7684\u539f\u56e0\u5728\u4e8e\r\n- `x = (x + self.positional_embedding).to(x.dtype)`\r\n\r\n```python\r\nimport types\r\nimport whisper\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\ndef whisper_encoder_forward_monkey_patch(self, x: torch.Tensor):\r\n\t'''\r\n\tx : torch.Tensor, shape = (batch_size, n_mels, n_ctx)\r\n\tthe mel spectrogram of the audio\r\n\t'''\r\n\tx = F.gelu(self.conv1(x))\r\n\tx = F.gelu(self.conv2(x))\r\n\tx = x.permute(0, 2, 1)\r\n\t# assert x.shape[1:] == self.positional_embedding.shape, 'incorrect audio shape'\r\n\t# x = (x + self.positional_embedding).to(x.dtype)\r\n\tx = (x + self.positional_embedding[: x.shape[1]]).to(x.dtype)\r\n\tfor block in self.blocks:\r\n\t\tx = block(x)\r\n\t\tx = self.ln_post(x)\r\n\treturn x\r\n```\r\n\r\n```python\r\n\r\nencoder = whisper.load_model('base').encoder\r\nencoder.whisper_encoder_forward_monkey_patch = types.MethodType(whisper_encoder_forward_monkey_patch, encoder)\r\naudio_path = ''\r\naudio = whisper.load_audio(audio_path)\r\nmel = whisper.log_mel_spectrogram(audio).to(model.device)\r\nfeatures = encoder.whisper_encoder_forward_monkey_patch(mel.unsqueeze(0))\r\n```\r\n\r\n```python\r\nwhisper.model.AudioEncoder.forward = forward\r\nmodel = whisper.load_model('')\r\naudio = whisper.load_audio('')\r\nmel = whisper.log_mel_spectrogram(audio,n_mels=model.dims.n_mels).to('cuda').to(model.device).unsquenze(0)\r\noutout = model.encoder(mel)\r\n```\r\n\u5982\u679c\u9700\u8981whisper\u63d0\u53d6\u51fa\u7684\u8be5\u7279\u5f81\u8fdb\u884c\u89e3\u7801\uff0c\u5fc5\u987b\u4f7f\u7528options\r\n\r\n```python\r\noptions = whisper.DecodingOptions(\r\n\u00a0 \u00a0 task='transcribe',\r\n\u00a0 \u00a0 language='zh',\r\n\u00a0 \u00a0 without_timestamps=True,\r\n\u00a0 \u00a0 beam_size=4,\r\n\r\n)\r\nprint(whisper.decode(model,mel,options))\r\n```\r\n\r\n## Whisper Options\r\n- `task`:\u9ed8\u8ba4\u4e3a`transcribe`\uff0c\u53ef\u4ee5\u8bbe\u7f6e\u4e3a`translate`,\u5373\u4e3a\u5c06\u8f93\u51fa\u7ffb\u8bd1\u4e3a\u82f1\u8bed\r\n\r\n\r\n\r\n## Huggingface\u7528\u6cd5\r\n```python\r\nfrom transformers import WhisperForConditionalGeneration, WhisperProcessor\r\n# \u52a0\u8f7d\u9884\u8bad\u7ec3\u7684Whisper\u6a21\u578b\u548c\u5904\u7406\u5668\r\nmodel = WhisperForConditionalGeneration.from_pretrained('openai/whisper-base')\r\nprocessor = WhisperProcessor.from_pretrained('openai/whisper-base')\r\n# \u5047\u8bbe\u4f60\u6709\u4e00\u4e2a\u8f93\u5165\u7684\u8bed\u97f3\u7279\u5f81\r\ninput_features = ...  # \u8fd9\u91cc\u5e94\u8be5\u662f\u9884\u5904\u7406\u540e\u7684\u8bed\u97f3\u7279\u5f81\r\n# \u5c06\u8f93\u5165\u7279\u5f81\u79fb\u52a8\u5230\u6a21\u578b\u6240\u5728\u7684\u8bbe\u5907\u4e0a\r\ninput_features = input_features.to(model.device)\r\n# \u4f7f\u7528\u5206\u5757\u7b97\u6cd5\u751f\u6210\u8f93\u51fa\r\noutputs = model.generate(\r\n    input_features=input_features,\r\n    return_dict_in_generate=True,\r\n    output_hidden_states=True,\r\n    chunk_length=30,  # \u8bbe\u7f6e\u5206\u5757\u957f\u5ea6\r\n    stride_length=15  # \u8bbe\u7f6e\u6b65\u957f\r\n)\r\n# \u89e3\u7801\u751f\u6210\u7684\u5e8f\u5217\r\ntranscriptions = processor.batch_decode(outputs.sequences, skip_special_tokens=True)[0]\r\n# \u6253\u5370\u8f6c\u5f55\u7ed3\u679c\r\nprint(transcriptions)\r\n\r\n```\u3002", "top": 0, "createdAt": 1733993645, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-12-12", "dateLabelColor": "#bc4c00"}, "P3": {"htmlDir": "docs/post/Pytorch-fu-xi-xi-lie-0--juan-shou-yu.html", "labels": ["QuickNote"], "postTitle": "Pytorch\u590d\u4e60\u7cfb\u52170:\u5377\u9996\u8bed", "postUrl": "post/Pytorch-fu-xi-xi-lie-0--juan-shou-yu.html", "postSourceUrl": "https://github.com/teamtee/teamtee.github.io/issues/3", "commentNum": 0, "wordCount": 658, "description": "# \u524d\u8a00\r\n\r\n\u5728\u52a8\u7b14\u5199\u4e0b\u8fd9\u7bc7\u7cfb\u5217\u7684\u7b2c\u4e00\u7bc7\u535a\u5ba2\u5f00\u59cb\uff0c\u6211\u5c31\u5fc5\u987b\u8981\u63d0\u9192\u81ea\u5df1\uff0c\u4e3a\u4ec0\u4e48\u8981\u5199\u4e0b\u300aPytorch\u5165\u95e8\u300b\u7684\u535a\u5ba2\uff0c\u5e02\u9762\u4e0a\u4e0d\u662f\u6709\u5f88\u591a\u7b14\u8bb0\u548c\u6559\u7a0b\u4e86\u5417\uff0c\u751a\u81f3\u4f60\u81ea\u5df1\u90fd\u662f\u901a\u8fc7\u8fd9\u4e9b\u8d44\u6599\u6765\u5165\u95e8Pytorch\u7684\uff0c\u8fd8\u9700\u8981\u4f60\u5199\u5165\u95e8\u6559\u7a0b\u5417\uff1f\r\n\r\n\u786e\u5b9e\u662f\u7684\uff0c\u6211\u60f3\u5e02\u9762\u4e0a\u7684\u8d44\u6599\u5df2\u7ecf\u5f88\u5168\u4e86\uff0c\u4f46\u662f\u6211\u89c9\u5f97\u8fd8\u6709\u4e00\u4e9b\u4e0d\u8db3\uff1a\r\n- \u7f3a\u4e4f\u7cfb\u7edf\u6027\uff1a\u7cfb\u7edf\u6027\u6307\u7684\u662f\u4e24\u4e2a\u65b9\u9762\uff0c\u77e5\u8bc6\u7684\u7ed3\u6784\u6027\u548c\u5c42\u6b21\u6027\uff0c\u5e02\u9762\u4e0a\u7684\u8d44\u6599\u5f80\u5f80\u662f\u5206\u6563\u7684\uff0c\u7f3a\u4e4f\u4ece\u4e00\u4e2a\u7cfb\u7edf\u7684\u89d2\u5ea6\u6765\u9610\u660e\u8981\u4e49\uff0c\u603b\u662f\u5c40\u9650\u4e8e\u67d0\u4e00\u79cd\u5e94\u7528\uff0c\u5e76\u4e14\u77e5\u8bc6\u5f80\u5f80\u4e0d\u5177\u5907\u5c42\u6b21\u6027\uff0c\u8981\u4e48\u8fc7\u6df1\uff0c\u8981\u4e48\u8fc7\u6d45\uff0c\u8981\u4e48\u8fc7\u5ea6\u7684\u96be\u5ea6\u8fc7\u4e8e\u9661\u5ced\u3002", "top": 0, "createdAt": 1734781125, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-12-21", "dateLabelColor": "#bc4c00"}, "P4": {"htmlDir": "docs/post/Pytorch-fu-xi-xi-lie-1-Tensor-zhang-liang.html", "labels": ["QuickNote"], "postTitle": "Pytorch\u590d\u4e60\u7cfb\u52171:Tensor\u5f20\u91cf", "postUrl": "post/Pytorch-fu-xi-xi-lie-1-Tensor-zhang-liang.html", "postSourceUrl": "https://github.com/teamtee/teamtee.github.io/issues/4", "commentNum": 0, "wordCount": 3003, "description": "# \u524d\u8a00\r\n\r\nPytorch\u8ba1\u7b97\u7684\u57fa\u672c\u5355\u4f4d\u5c31\u662fTensor,\u4e2d\u6587\u540d\u5f20\u91cf\u3002", "top": 0, "createdAt": 1734781247, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-12-21", "dateLabelColor": "#bc4c00"}, "P5": {"htmlDir": "docs/post/Pytorch-fu-xi-xi-lie-2-Dataset-shu-ju-ji.html", "labels": ["QuickNote"], "postTitle": "Pytorch\u590d\u4e60\u7cfb\u52172:Dataset\u6570\u636e\u96c6", "postUrl": "post/Pytorch-fu-xi-xi-lie-2-Dataset-shu-ju-ji.html", "postSourceUrl": "https://github.com/teamtee/teamtee.github.io/issues/5", "commentNum": 0, "wordCount": 6053, "description": "# \u524d\u8a00\r\n\r\nDataset\u662f\u5b58\u50a8\u6570\u636e\u7684\u96c6\u5408\uff0c\u3002", "top": 0, "createdAt": 1734790578, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-12-21", "dateLabelColor": "#bc4c00"}, "P6": {"htmlDir": "docs/post/Pytorch-fu-xi-xi-lie-3-nn.Parameters-can-shu.html", "labels": ["QuickNote"], "postTitle": "Pytorch\u590d\u4e60\u7cfb\u52173:nn.Parameters\u53c2\u6570", "postUrl": "post/Pytorch-fu-xi-xi-lie-3-nn.Parameters-can-shu.html", "postSourceUrl": "https://github.com/teamtee/teamtee.github.io/issues/6", "commentNum": 0, "wordCount": 2362, "description": "# \u524d\u8a00\r\n\r\nParameter\u548cBuffer\u90fd\u662f\u5b9e\u4f8b\u5316\u7684Tensor\uff0cParameter\u662f\u53c2\u4e0e\u68af\u5ea6\u8fd0\u7b97\u7684\u53c2\u6570\uff0cBuffer\u662f\u4e0d\u53c2\u4e0e\u68af\u5ea6\u8ba1\u7b97\u7684\u53c2\u6570\r\n\r\n`class Parameter(torch.Tensor, metaclass=_ParameterMeta):`\r\n\r\n- `Parameter`\u00a0\u662f\u4e00\u4e2a\u7279\u6b8a\u7684\u5f20\u91cf\uff0c\u5b83\u88ab\u7528\u6765\u8868\u793a\u6a21\u578b\u7684\u53c2\u6570,\u81ea\u52a8\u5c06\u00a0`Parameter`\u00a0\r\n\r\n `class Buffer(torch.Tensor, metaclass=_BufferMeta):`\r\n \r\n- `Buffer`\u00a0\u4e5f\u662f\u4e00\u4e2a\u7279\u6b8a\u7684\u5f20\u91cf\uff0c\u5b83\u7528\u4e8e\u5b58\u50a8\u90a3\u4e9b\u5728\u6a21\u578b\u4e2d\u4e0d\u76f4\u63a5\u53c2\u4e0e\u68af\u5ea6\u8ba1\u7b97\u7684\u6570\u636e\uff0c\u4f46\u53ef\u80fd\u5728\u6a21\u578b\u7684\u524d\u5411\u6216\u540e\u5411\u4f20\u64ad\u4e2d\u4f7f\u7528\u3002", "top": 0, "createdAt": 1735008754, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-12-24", "dateLabelColor": "#bc4c00"}, "P7": {"htmlDir": "docs/post/Pytorch-fu-xi-xi-lie-4-nn.Modules-mo-kuai.html", "labels": ["QuickNote"], "postTitle": "Pytorch\u590d\u4e60\u7cfb\u52174:nn.Modules\u6a21\u5757", "postUrl": "post/Pytorch-fu-xi-xi-lie-4-nn.Modules-mo-kuai.html", "postSourceUrl": "https://github.com/teamtee/teamtee.github.io/issues/7", "commentNum": 0, "wordCount": 12802, "description": "# \u524d\u8a00\r\nnn.Modules\u4e0b\u9762\u5305\u542b\u5f88\u591ann.Module\u7684\u5b9e\u4f8b\uff0cnn.Module\u662fPytorch\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u7684\u7236\u7c7b\r\n\u53c2\u8003\r\n[1.PyTorch \u6e90\u7801\u89e3\u8bfb\u4e4b nn.Module\uff1a\u6838\u5fc3\u7f51\u7edc\u6a21\u5757\u63a5\u53e3\u8be6\u89e3](https://zhuanlan.zhihu.com/p/340453841)\r\n\r\n# nn.Module\u57fa\u672c\u5c5e\u6027\r\n\u5728Module\u7684__init__\u51fd\u6570\u4e2d\u53ef\u80fd\u89c2\u5bdf\u5230\u4e0b\u9762nn.Modules\u7684\u6838\u5fc3\u7ec4\u4ef6\r\n\r\n```python\r\nself.training = True  # \u63a7\u5236 training/testing \u72b6\u6001\r\nself._parameters = OrderedDict()  # \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f1a\u968f\u7740 BP \u800c\u66f4\u65b0\u7684\u53c2\u6570\r\nself._buffers = OrderedDict()  # \u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e0d\u4f1a\u968f\u7740 BP \u800c\u66f4\u65b0\u7684\u53c2\u6570\r\nself._non_persistent_buffers_set = set()\r\nself._backward_hooks = OrderedDict()  # Backward \u5b8c\u6210\u540e\u4f1a\u88ab\u8c03\u7528\u7684 hook\r\nself._forward_hooks = OrderedDict()  # Forward \u5b8c\u6210\u540e\u4f1a\u88ab\u8c03\u7528\u7684 hook\r\nself._forward_pre_hooks = OrderedDict()  # Forward \u524d\u4f1a\u88ab\u8c03\u7528\u7684 hook\r\nself._state_dict_hooks = OrderedDict()  # \u5f97\u5230 state_dict \u4ee5\u540e\u4f1a\u88ab\u8c03\u7528\u7684 hook\r\nself._load_state_dict_pre_hooks = OrderedDict()  # load state_dict \u524d\u4f1a\u88ab\u8c03\u7528\u7684 hook\r\nself._modules = OrderedDict()  # \u5b50\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\r\n```\r\n\r\n## \u57fa\u672c\u5c5e\u6027\r\n\u4e0b\u9762\u7684\u51fd\u6570\u53ef\u4ee5\u83b7\u53d6\u8fd9\u4e9b\u53c2\u6570\r\n- named_parameters\uff1a\u8fd4\u56de\u81ea\u8eabparameters,\u5982\u679c recurse=True \u8fd8\u4f1a\u8fd4\u56de\u5b50\u6a21\u5757\u4e2d\u7684\u6a21\u578b\u53c2\u6570\r\n- named_buffers\uff1a\u8fd4\u56de\u81ea\u8eabparameters,\u5982\u679c recurse=True \u8fd8\u4f1a\u8fd4\u56de\u5b50\u6a21\u5757\u4e2d\u7684\u6a21\u578b buffer\r\n- named_children\uff1a\u8fd4\u56de\u81ea\u8eab\u7684Modules\r\n-  named_modules\uff1a\u8fd4\u56de\u81ea\u8eab\u548c\u5b50Modules\u7684Moduels(\u9012\u5f52\u8c03\u7528)\r\n\r\n\u4e0b\u9762\u7684\u53c2\u6570\u662f\u5bf9\u4e0a\u9762\u7684\u8c03\u7528,\u9ed8\u8ba4recurse\u53c2\u6570\u4e3aTrue\r\n- parameters\uff1a\r\n-  buffers\uff1a\r\n-  children\uff1a\r\n-  modules\uff1a\r\n\u6dfb\u52a0\u53c2\u6570\r\n- add_module\uff1a\u589e\u52a0\u5b50\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\uff0c\u66f4\u65b0 self._modules\r\n```\r\nadd_module(name,module)\r\n```\r\n-  register_parameter\uff1a\u589e\u52a0\u901a\u8fc7 BP \u53ef\u4ee5\u66f4\u65b0\u7684 parameters \uff08\u5982 BN \u548c Conv \u4e2d\u7684 weight \u548c bias \uff09\uff0c\u66f4\u65b0 self._parameters\r\n- register_buffer\uff1a\u589e\u52a0\u4e0d\u901a\u8fc7 BP \u66f4\u65b0\u7684 buffer\uff08\u5982 BN \u4e2d\u7684 running_mean \u548c running_var\uff09\r\n- self.xxx = xxx \uff1a\u8be5\u65b9\u6cd5\u4e0d\u4f1a\u88ab\u767b\u8bb0\uff0c\u4e0d\u5c5e\u4e8eParamets\u548cbuffer\uff0c\u8fdb\u884c\u72b6\u6001\u8f6c\u6362\u7684\u65f6\u5019\u4f1a\u88ab\u9057\u6f0f\r\n\u4e0b\u9762\u7684\u51fd\u6570\u53ef\u4ee5\u8c03\u6574\u68af\u5ea6\r\n- train()\r\n- eval()\r\n- requires_grad_()\r\n- zero_gred()\r\n\r\n\u4e0b\u9762\u7684\u51fd\u6570\u53ef\u4ee5\u6620\u5c04parameters\u548cbuffers\r\n- `_apply(fn)`:\u9488\u5bf9parameters\u548cbuffers\u901a\u8fc7\u8c03\u7528\u6240\u6709parameters\u548cbuffers\u7684tensor\u7684_apply\u51fd\u6570\u5b9e\u73b0\r\n\r\n```\r\n1. CPU\uff1a\u5c06\u6240\u6709 parameters \u548c buffer \u8f6c\u79fb\u5230 CPU \u4e0a\r\n2. type\uff1a\u5c06\u6240\u6709 parameters \u548c buffer \u8f6c\u53d8\u6210\u53e6\u4e00\u4e2a\u7c7b\u578b\r\n3. CUDA\uff1a\u5c06\u6240\u6709 parameters \u548c buffer \u8f6c\u79fb\u5230 GPU \u4e0a\r\n4. float\uff1a\u5c06\u6240\u6709\u6d6e\u70b9\u7c7b\u578b\u7684 parameters \u548c buffer \u8f6c\u53d8\u6210\u00a0float32\r\n5. double\uff1a\u5c06\u6240\u6709\u6d6e\u70b9\u7c7b\u578b\u7684 parameters \u548c buffer \u8f6c\u53d8\u6210 double \u7c7b\u578b\r\n6. half\uff1a\u5c06\u6240\u6709\u6d6e\u70b9\u7c7b\u578b\u7684 parameters \u548c buffer \u8f6c\u53d8\u6210 float16 \u7c7b\u578b\r\n8. to\uff1a\u79fb\u52a8\u6a21\u5757\u6216/\u548c\u6539\u53d8\u6a21\u5757\u7684\u7c7b\u578b\r\n```\r\n- `apply`:\u9488\u5bf9Moduels\uff0c\r\n\u53ef\u4ee5\u81ea\u5b9a\u4e49\u4e00\u4e2a init_weights \u51fd\u6570\uff0c\u901a\u8fc7\u00a0`net.apply(init_weights)`\u00a0\u6765\u521d\u59cb\u5316\u6a21\u578b\u6743\u91cd\u3002", "top": 0, "createdAt": 1735023260, "style": "<style>.markdown-alert{padding:0.5rem 1rem;margin-bottom:1rem;border-left:.25em solid var(--borderColor-default,var(--color-border-default));}.markdown-alert .markdown-alert-title {display:flex;font-weight:var(--base-text-weight-medium,500);align-items:center;line-height:1;}.markdown-alert>:first-child {margin-top:0;}.markdown-alert>:last-child {margin-bottom:0;}</style><style>.markdown-alert.markdown-alert-note {border-left-color:var(--borderColor-accent-emphasis, var(--color-accent-emphasis));background-color:var(--color-accent-subtle);}.markdown-alert.markdown-alert-note .markdown-alert-title {color: var(--fgColor-accent,var(--color-accent-fg));}</style>", "script": "<script>MathJax = {tex: {inlineMath: [[\"$\", \"$\"]]}};</script><script async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-12-24", "dateLabelColor": "#bc4c00"}, "P8": {"htmlDir": "docs/post/Pytorch-fu-xi-xi-lie-5-torch.cuda.amp.html", "labels": ["QuickNote"], "postTitle": "Pytorch\u590d\u4e60\u7cfb\u52175:torch.cuda.amp", "postUrl": "post/Pytorch-fu-xi-xi-lie-5-torch.cuda.amp.html", "postSourceUrl": "https://github.com/teamtee/teamtee.github.io/issues/8", "commentNum": 0, "wordCount": 3209, "description": "# \u524d\u8a00\r\n\r\n\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u7684\u6838\u5fc3\u89c2\u70b9\uff1a**\u91c7\u7528\u66f4\u4f4e\u7cbe\u5ea6\u7684\u7c7b\u578b\u8fdb\u884c\u8fd0\u7b97\u4f1a\u4f7f\u7528\u66f4\u5c11\u7684\u5185\u5b58\u548c\u66f4\u5feb\u7684\u901f\u5ea6**\r\n\u5fc5\u987b\u91c7\u7528Tensor core\u7684\u6838\u5fc3\u663e\u5361\uff1a\u00a0GPU \u4e2d\u7684 Tensor Core \u5929\u7136\u652f\u6301 FP16 \u4e58\u79ef\u7684\u7ed3\u679c\u4e0e FP32 \u7684\u7d2f\u52a0\r\n## \u539f\u7406\r\n[Mixed Precision Training](https://arxiv.org/abs/1710.03740)\r\n\r\n[\u6709\u5173\u53c2\u6570\u7684\u8bb2\u89e3\u7684\u597d\u6587\u7ae0](https://www.53ai.com/news/finetuning/2024083051493.html)\r\n[\u6709\u5173torch.cuda.amp\u7684\u597d\u6587\u7ae0](https://zhuanlan.zhihu.com/p/348554267)\r\n[\u8bb2\u89e3DeepSpeed\u7684\u597d\u6587\u7ae0](https://basicv8vc.github.io/posts/zero/)\r\n[\u6709\u5173FSDP\u5185\u5b58\u6d88\u8017\u7684\u7edd\u4e16\u597d\u6587\u7ae0](https://cloud.tencent.com/developer/article/2314837)\r\n## \u53c2\u6570\u7c7b\u578b\r\n\u6a21\u578b\u5728\u4fdd\u5b58\u7684\u65f6\u5019\u901a\u5e38\u6709\u4e0b\u9762\u56db\u79cd\u7c7b\u578b\r\n- fp32\r\n- tf32\r\n- fp16\r\n- bf16\r\n![image](https://github.com/user-attachments/assets/2211ccf2-b62a-4de8-8eac-4fbda5d599cc)\r\n\u6211\u4eec\u9700\u8981\u533a\u5206\u4e0b\u9762\u7684\u6982\u5ff5\uff0c\u4fdd\u5b58\u7c7b\u578b\u901a\u5e38\u65f6\u9884\u8bad\u7ec3\u6a21\u578b\u5df2\u7ecf\u6307\u5b9a\u597d\u7684\uff0c\u52a0\u8f7d\u7c7b\u578b\u6211\u4eec\u53ef\u4ee5\u6307\u5b9a\uff0c\u5728\u8fd0\u7b97\u65f6\u6a21\u578b\u4f1a\u81ea\u52a8\u5c06\u5c06\u8fd0\u7b97\u7684\u7c7b\u578b\u8f6c\u6362\u4e3a\u6a21\u578b\u7684\u52a0\u8f7d\u7c7b\u578b\r\n\r\n- \u4fdd\u5b58\u7c7b\u578b\uff1a\r\n- \u52a0\u8f7d\u7c7b\u578b\uff1a\r\n- \u8fd0\u7b97\u7c7b\u578b:\r\n\u6307\u5b9a\u52a0\u8f7d\u7c7b\u578b\r\n```python\r\nfrom transformers import AutoModel\r\n# \u52a0\u8f7d\u6a21\u578b\u65f6\u6307\u5b9a\u53c2\u6570\u7c7b\u578b\u4e3afloat16\r\nmodel = AutoModel.from_pretrained('bert-base-uncased', torch_dtype=torch.float16)\r\n# \u6a21\u578b\u8fd0\u7b97\u65f6\uff0c\u5982\u679c\u4f7f\u7528GPU\uff0c\u4f1a\u81ea\u52a8\u4f7f\u7528\u5bf9\u5e94\u7684\u53c2\u6570\u7c7b\u578b\u8fdb\u884c\u8ba1\u7b97\r\n# \u4f8b\u5982\uff0c\u5728NVIDIA GPU\u4e0a\uff0cfloat16\u8fd0\u7b97\u4f1a\u4f7f\u7528Tensor Cores\u52a0\u901f\r\n```\r\n\u6307\u5b9a\u52a0\u8f7d\u7c7b\u578b\uff0c\u5e76\u4e14\u91cf\u5316\r\n```python\r\nfrom transformers import AutoModel\r\nfrom bitsandbytes as bnb\r\n\r\n# \u6307\u5b9a\u91cf\u5316\u914d\u7f6e\r\n\u91cf\u5316\u914d\u7f6e = bnb.QuantizationConfig(\r\n    load_in_8bit=True,\r\n    bnb_8bit_quant_type='nf4',\r\n    bnb_8bit_use_double_quant=False,\r\n)\r\n\r\n# \u52a0\u8f7d\u5e76\u91cf\u5316\u6a21\u578b\r\nmodel = AutoModel.from_pretrained(\r\n    'bert-base-uncased',\r\n    quantization_config=\u91cf\u5316\u914d\u7f6e,\r\n)\r\n```\r\n\u6df7\u5408\u7cbe\u5ea6\u8fd0\u7b97\u7684\u6838\u5fc3\u601d\u60f3\uff1a\u91c7\u7528\u8f83\u9ad8\u7cbe\u5ea6\u7684\u53c2\u6570\u7c7b\u578b\u52a0\u8f7d\u6a21\u578b\uff0c\u4f46\u662f\u8fd0\u7b97\u65f6\u5c06\u4e00\u4e9b\u8fd0\u7b97\u8f6c\u5316\u4e3a\u4f4e\u7cbe\u5ea6\u7684\u53c2\u6570\u7c7b\u578b\u6765\u52a0\u5feb**\u8bad\u7ec3\u548c\u8fd0\u7b97**\uff0c\u5177\u4f53\u8f6c\u5316\u4ec0\u4e48\u7b97\u5b50\u7531pytorch\u81ea\u52a8\u51b3\u5b9a\u3002", "top": 0, "createdAt": 1735626812, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2024-12-31", "dateLabelColor": "#bc4c00"}, "P10": {"htmlDir": "docs/post/zheng-ze-biao-da-shi.html", "labels": ["QuickNote"], "postTitle": "\u6b63\u5219\u8868\u8fbe\u5f0f", "postUrl": "post/zheng-ze-biao-da-shi.html", "postSourceUrl": "https://github.com/teamtee/teamtee.github.io/issues/10", "commentNum": 0, "wordCount": 3294, "description": "# \u7b80\u4ecb\r\n\u6b63\u5219\u8868\u8fbe\u5f0f\u662f\u4e00\u95e8\u5728\u53d1\u5c55\u4e2d\u9010\u6e10\u5f62\u6210\u7684\u5b66\u95ee\uff0c\u56e0\u6b64\u5b58\u5728\u5386\u53f2\u7248\u672c\u7684\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u6709\u5f88\u591a\u89c4\u8303\uff0c\u73b0\u5728\u6211\u4eec\u4f7f\u7528\u7684\u89c4\u8303\u57fa\u672c\u90fd\u548cPOSIX\u7684\u89c4\u8303\u4e00\u81f4\u3002", "top": 0, "createdAt": 1735893107, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-01-03", "dateLabelColor": "#0969da"}, "P11": {"htmlDir": "docs/post/Linux-xia-chang-yong-hao-yong-ming-ling-zong-jie.html", "labels": ["QuickNote"], "postTitle": "Linux\u4e0b\u5e38\u7528\u597d\u7528\u547d\u4ee4\u603b\u7ed3", "postUrl": "post/Linux-xia-chang-yong-hao-yong-ming-ling-zong-jie.html", "postSourceUrl": "https://github.com/teamtee/teamtee.github.io/issues/11", "commentNum": 0, "wordCount": 449, "description": "\u547d\u4ee4\u884c\u7684\u53c2\u6570\u7e41\u591a\uff0c\u4f46\u662f\u6211\u4eec\u5e38\u5e38\u4f7f\u7528\u7684\u4e0d\u8fc7\u5be5\u5be5\uff0c\u56e0\u6b64\u6211\u5c06\u5e38\u7528\u7684\u547d\u4ee4\u884c\u7528\u6cd5\u7f57\u5217\u5982\u4e0b\r\n## Linux\r\n## find\r\n\r\n```bash\r\nfind . -name '*ext' -o -name '*pattern*'\r\n```\r\n- -o\u8868\u793a\u52a0\u4e0a\u53e6\u4e00\u4e2a\u67e5\u627e\u9879\u76ee\r\n## sed\r\n```bash\r\nsed -i 's::g' [filename]\r\n```\r\n- -i\u8868\u793a\u76f4\u63a5\u4fee\u6539\u539f\u6587\u4ef6\u4e0d\u8f93\u51fa\r\n## paste\r\n\u6309\u7167\u884c\u62fc\u63a5\u4e24\u4e2a\u6587\u4ef6\r\n```bash\r\npaste -d '' file1 file2\r\n```\r\n- -d \u6307\u5b9a\u5206\u5272\u7b26\r\n## split\r\n\u6309\u7167\u884c\u6570\u7ec6\u5206\u6587\u4ef6\r\n```bash\r\nsplit -n 1000 file\r\nsplit -n 100 -d file \r\n```\r\n- -d\u8868\u793a\u901a\u8fc7\u6570\u5b57\u547d\u4ee4\u5b50\u6587\u4ef6\uff0c\u9ed8\u8ba4\u7528\u5b57\u6bcd\r\n\r\n## Python\r\n\r\n### torch\r\n- python -m torch.utils.collect_env\r\n\u6536\u96c6\u5f62\u6210\u8be6\u7ec6\u7684\u73af\u5883\u4fe1\u606f\r\n\u3002", "top": 0, "createdAt": 1735893344, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-01-03", "dateLabelColor": "#0969da"}, "P12": {"htmlDir": "docs/post/Qwen-Audio-jie-du-xi-lie.html", "labels": ["QuickNote"], "postTitle": "Qwen-Audio\u89e3\u8bfb\u7cfb\u5217", "postUrl": "post/Qwen-Audio-jie-du-xi-lie.html", "postSourceUrl": "https://github.com/teamtee/teamtee.github.io/issues/12", "commentNum": 0, "wordCount": 1423, "description": "# \u7b80\u4ecb\n\nQwen-Audio\u662f\u7531\u963f\u91cc\u7684Qwen\u56e2\u961f\u5f00\u53d1\u7684\u8bed\u97f3\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u5206\u4e3aQwen-Audio\uff0cQwen2-Audio,\u4e24\u8005\u5747\u5177\u5907\u5f3a\u5927\u7684\u542c\u89c9\u80fd\u529b\uff0c\u5177\u5907\u5904\u7406\u8bed\u97f3\u3001\u97f3\u9891\u4e8b\u4ef6\u548c\u97f3\u4e50\u6b4c\u66f2\u7684\u5e7f\u6cdb\u80fd\u529b\n\nPaper\uff1a[Qwen-Audio](https://arxiv.org/abs/2311.07919)\nGithub\uff1a[Qwen-Audio](https://github.com/QwenLM/Qwen-Audio)\nPaper\uff1a[Qwen2-Audio](https://arxiv.org/abs/2407.10759)\nGithub\uff1a[Qwen2-Audio](https://github.com/QwenLM/Qwen2-Audio)\n\uff08\u6ce8\u91ca\uff1aQwen-Audio\u7684\u4ee3\u7801\u6bd4Qwen2-Audio\u8be6\u7ec6\u5f88\u591a\uff09\n## \u6570\u636e\n\nQwen\n\n![Image](https://github.com/user-attachments/assets/2d2e32e9-ff90-45b4-a828-8665cfbd4e71)\n\n- Speech\uff1a\u7ea689.2k(\u4e0d\u8db31k\u6309\u71671k\u7684\u8ba1\u7b97)\n- Sound\uff1a\u7ea616.8k\n- Music\uff1a\u7ea640.5k\n\nQwen2-Audio\n- Spech\uff1a37k\n- Sound\uff1a10k\n- Music\uff1a140k\n## \u6a21\u578b\nQwen-Audio\uff1aQwen-Audio\u7684\u5b9e\u9645\u7ed3\u6784\u4e3a\u00a0Whisper-large-v2+\u7ebf\u6027\u5c42\u7ec4\u5408+Qwen-7B(\u5177\u4f53\u578b\u53f7\u6ca1\u627e\u5230)\n\n\u7ebf\u6027\u5c42\u7ec4\u5408 = Linear3 * \uff08\uff08linear2* speech_tokens\uff09 * F.silu(linear1* speech_tokens)\uff09\n![Image](https://github.com/user-attachments/assets/bc90e9ea-03b4-4169-8e5c-2ceb00658cc1)\n\nQwen2-Audio:\u5b9e\u9645\u7ed3\u6784\u4e3aWhisperlarge-v3+\u5e73\u5747\u6c60\u5316\u5c42(\u957f2)+\u7ebf\u6027\u5c42+Qwen-7B(\u5177\u4f53\u578b\u53f7\u6ca1\u627e\u5230)\n![Image](https://github.com/user-attachments/assets/6319993a-5269-4a2b-82cc-00ed24b690e7)\n## \u8bad\u7ec3\nQwen-audio:Qwen-Audio\u91c7\u7528\u7684\u662f\u7c7b\u4f3cWhisper\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u5373\u4e3a\u9884\u6d4bToken\uff0c\u5e76\u4e14Qwen-Audio\u53ea\u8bad\u7ec3Whisper+MLP\uff0c\u4e0d\u5bf9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u800c\u4e14\u53ea\u662f\u91c7\u7528\u9884\u8bad\u7ec3\uff0cSFT\u5f97\u5230\u7684\u662fQwen-Audio-Chat\n\nQwen2-audio:\u91c7\u7528Pretrain+SFT+DPO\uff0c\u5177\u4f53\u8bad\u7ec3\u7684\u6a21\u578b\u54ea\u90e8\u5206\u6ca1\u63d0\u5230\n- Pretrain\uff1a\u8bad\u7ec3ASR+ACC\n- SFT\uff1a\u5206\u4e3a\u4e24\u7c7b\u4efb\u52a1\uff0c\u8bed\u97f3\u5206\u6790\u4efb\u52a1\uff1a\u6587\u672c\u6307\u4ee4+\u8bed\u97f3\uff0c\u804a\u5929\u4efb\u52a1\uff1a\u8bed\u97f3\n- DPO\uff1a\u4f18\u5316\u8868\u73b0\n## \u7ed3\u679c\nQwen-Audio\n\n![Image](https://github.com/user-attachments/assets/781b1d46-34a6-46fb-ab4e-bdc066b3a0fd)\n\nQwen2-Audio\n\n![Image](https://github.com/user-attachments/assets/ee9a8c6c-42c7-4e6c-a6f9-abff8216c6b1)\u3002", "top": 0, "createdAt": 1739265713, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-02-11", "dateLabelColor": "#0969da"}, "P13": {"htmlDir": "docs/post/Pytorch-ku-de-shi-yong-torchaudio.html", "labels": ["QuickNote"], "postTitle": "Pytorch\u5e93\u7684\u4f7f\u7528torchaudio", "postUrl": "post/Pytorch-ku-de-shi-yong-torchaudio.html", "postSourceUrl": "https://github.com/teamtee/teamtee.github.io/issues/13", "commentNum": 0, "wordCount": 7669, "description": "\n[\u5b98\u7f51\u6559\u7a0b](https://pytorch.org/audio/main/)\n## \u524d\u8a00\n\ntorchaudio.transformers\u662fPytorch\u5b98\u65b9\u63d0\u4f9b\u7684\u6709\u5173\u97f3\u9891\u5904\u7406\u7684\u5e93\n\n### \u8bfb\u5199\n\u52a0\u8f7d\u4e0e\u4fdd\u5b58\n- torchaudio.info(path)\n- torchaudio.load(path)\n- torchaudio.save(path,wavform,sample_rate)\n### StreamReader & StreamWriter(\u7565)\n\u53ef\u7528\u4e8e\u6d41\u5f0f\u7684\u8bfb\u5199\uff0c\u652f\u6301\u9ea6\u514b\u98ce\uff0c\u7f51\u7edc\u8bfb\u5199\n### transformers\n```\nimport torchaudio\nimport torchaudio.functional as F\nimport torchaudio.transforms as T\n```\n### Resample\n\u8981\u5c06\u97f3\u9891\u6ce2\u5f62\u4ece\u4e00\u4e2afreqeeuncy\u91cd\u65b0\u91c7\u6837\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 torchaudio.transforms.Resample \u6216\u8005 torchaudio.functional.resample() \n```\nresampler = T.Resample(sample_rate, resample_rate, dtype=waveform.dtype)\nresampled_waveform = resampler(waveform)\n```\n\n```\nresampled_waveform = F.resample(waveform, sample_rate, resample_rate, lowpass_filter_width=6)\n```\n### [Data Augmentation](https://pytorch.org/audio/stable/tutorials/audio_data_augmentation_tutorial.html)\n\n#### \u00a0filters\u6ee4\u6ce2\u5668\n\n```python\n# Define effects\neffect = ','.join(\n    [\n        'lowpass=frequency=300:poles=1',  # apply single-pole lowpass filter\n        'atempo=0.8',  # reduce the speed\n        'aecho=in_gain=0.8:out_gain=0.9:delays=200:decays=0.3|delays=400:decays=0.3'\n        # Applying echo gives some dramatic feeling\n    ],\n)\neffector = torchaudio.io.AudioEffector(effect=effect)\neffector.apply(waveform, sample_rate)\n```\n\n#### RIR\u6df7\u54cd\u6a21\u62df\n![[rir.wav]]\n```python\nrir_raw, sample_rate = torchaudio.load(SAMPLE_RIR)\nrir = rir_raw[:, int(sample_rate * 1.01) : int(sample_rate * 1.3)]\nrir = rir / torch.linalg.vector_norm(rir, ord=2)\nspeech, _ = torchaudio.load(SAMPLE_SPEECH)\naugmented = F.fftconvolve(speech, rir)\n```\n#### \u6dfb\u52a0\u566a\u58f0\n```python\nspeech, _ = torchaudio.load(SAMPLE_SPEECH)\nnoise, _ = torchaudio.load(SAMPLE_NOISE)\nnoise = noise[:, : speech.shape[1]]\n\nsnr_dbs = torch.tensor([20, 10, 3])\nnoisy_speeches = F.add_noise(speech, noise, snr_dbs)\n```\n\n```python\nimport torch\n\nimport torchaudio\n\nimport torchaudio.functional as F\n\nimport random\n\n  \n\n# \u52a0\u8f7d\u8bed\u97f3\u548c\u566a\u58f0\u6587\u4ef6\n\nspeech, _ = torchaudio.load('/hpc_stor01/home/yangui.fang_sx/workingspace/tools/R8009_M8024-8076-000350_000545.flac')\n\nnoise, _ = torchaudio.load('/hpc_stor01/home/yangui.fang_sx/workingspace/tools/rir.wav')\n\n# \u786e\u4fdd\u8bed\u97f3\u548c\u566a\u58f0\u7684\u91c7\u6837\u7387\u76f8\u540c\n\nassert fs == noise.shape[1], '\u8bed\u97f3\u548c\u566a\u58f0\u7684\u91c7\u6837\u7387\u5fc5\u987b\u76f8\u540c'\n\n  \n\n# \u5982\u679c\u8bed\u97f3\u6bd4\u566a\u58f0\u957f\uff0c\u968f\u673a\u9009\u62e9\u566a\u58f0\u7684\u8d77\u59cb\u70b9\n\nif speech.shape[1] > noise.shape[1]:\n\n\u00a0 \u00a0 # \u968f\u673a\u9009\u62e9\u566a\u58f0\u7684\u8d77\u59cb\u70b9\n\n\u00a0 \u00a0 start_idx = random.randint(0, speech.shape[1] - noise.shape[1])\n\n\u00a0 \u00a0 # \u5728\u8bed\u97f3\u7684\u968f\u673a\u4f4d\u7f6e\u5f00\u59cb\u6dfb\u52a0\u566a\u58f0\n\n\u00a0 \u00a0 speech_with_noise = speech.clone()\n\n\u00a0 \u00a0 speech_with_noise[:, start_idx:start_idx + noise.shape[1]] += noise\n\nelse:\n\n\u00a0 \u00a0 # \u5982\u679c\u566a\u58f0\u6bd4\u8bed\u97f3\u957f\uff0c\u4ece\u566a\u58f0\u7684\u968f\u673a\u4f4d\u7f6e\u5f00\u59cb\u622a\u53d6\n\n\u00a0 \u00a0 start_idx = random.randint(0, noise.shape[1] - speech.shape[1])\n\n\u00a0 \u00a0 noise = noise[:, start_idx:start_idx + speech.shape[1]]\n\n\u00a0 \u00a0 # \u76f4\u63a5\u5c06\u566a\u58f0\u6dfb\u52a0\u5230\u8bed\u97f3\u4e2d\n\n\u00a0 \u00a0 snr_dbs = random.randomint(1, 30)\n\n\u00a0 \u00a0 noisy_speeches = F.add_noise(speech, noise, snr_dbs)\n\n  \n\n# \u4fdd\u5b58\u5e26\u566a\u8bed\u97f3\u4fe1\u53f7\n\nAudio(speech_with_noise, rate=fs)\n\n# output_path = 'noisy_speech.wav'\n\n# torchaudio.save(output_path, speech_with_noise, fs)\n\n# print(f'Saved noisy speech to {output_path}')\n```\n#### \u7f16\u7801\n\n```python\nencoder = torchaudio.io.AudioEffector(format=format, encoder=encoder)\nencoder.apply(waveform, sample_rate)\t\n```\n format,encoder\u652f\u6301\u4e0b\u9762\u7684\uff0c\n- 'wav','pcm_mulaw'\n- 'g722'\n- 'ogg', encoder='vorbis'\n\n[### Feature Extract \u7279\u5f81\u63d0\u53d6](https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html)\n\n#### Spectrogram\n\n```python\nspectrogram = T.Spectrogram(n_fft=512)\n\n# Perform transform\nspec = spectrogram(wav)\n```\n#### GriffinLim \n\u57fa\u4e8e\u89c4\u5219\u7684\u4ece\u9891\u8c31\u6062\u590d\u6ce2\u5f62\n\n```python\n# Define transforms\nn_fft = 1024\nspectrogram = T.Spectrogram(n_fft=n_fft)\ngriffin_lim = T.GriffinLim(n_fft=n_fft)\n\n# Apply the transforms\nspec = spectrogram(SPEECH_WAVEFORM)\nreconstructed_waveform = griffin_lim(spec)\n```\n#### Melspectrogram\n\n```python\nn_fft = 1024\nwin_length = None\nhop_length = 512\nn_mels = 128\n\nmel_spectrogram = T.MelSpectrogram(\n    sample_rate=sample_rate,\n    n_fft=n_fft,\n    win_length=win_length,\n    hop_length=hop_length,\n    center=True,\n    pad_mode='reflect',\n    power=2.0,\n    norm='slaney',\n    n_mels=n_mels,\n    mel_scale='htk',\n)\n\nmelspec = mel_spectrogram(SPEECH_WAVEFORM)\n```\n\n#### MFCC\n\n```python\nn_fft = 2048\nwin_length = None\nhop_length = 512\nn_mels = 256\nn_mfcc = 256\n\nmfcc_transform = T.MFCC(\n    sample_rate=sample_rate,\n    n_mfcc=n_mfcc,\n    melkwargs={\n        'n_fft': n_fft,\n        'n_mels': n_mels,\n        'hop_length': hop_length,\n        'mel_scale': 'htk',\n    },\n)\n\nmfcc = mfcc_transform(SPEECH_WAVEFORM)\n```\n#### LFCC\n```python\nn_fft = 2048\nwin_length = None\nhop_length = 512\nn_lfcc = 256\n\nlfcc_transform = T.LFCC(\n    sample_rate=sample_rate,\n    n_lfcc=n_lfcc,\n    speckwargs={\n        'n_fft': n_fft,\n        'win_length': win_length,\n        'hop_length': hop_length,\n    },\n)\n\nlfcc = lfcc_transform(SPEECH_WAVEFORM)\nplot_spectrogram(lfcc[0], title='LFCC')\n```\n\n#### Picth \u97f3\u9ad8\n\n```python\npitch = F.detect_pitch_frequency(SPEECH_WAVEFORM, SAMPLE_RATE)\n```\n\n\n\n#### SpecAugment\n\n```python\n\nspec = spec.permute(1, 0).unsqueeze(0)\nstretch = T.Spectrogram(n_freq=\u901a\u9053\u6570)\nrate = random.random()*0.2 + 0.9\nspec = stretch(spec, rate)\n\nTimemasking = T.TimeMasking(time_mask_param=100)\nFrequencymasking = T.FrequencyMasking(freq_mask_param=27)\nspec = Timemasking(spec)\nspec = Timemasking(spec)\nspec = Frequencymasking(spec)\nspec = Frequencymasking(spec)  \n```\n\n### [CTC\u5f3a\u5236\u5bf9\u9f50](https://pytorch.org/audio/stable/tutorials/ctc_forced_alignment_api_tutorial.html)\n\n- torchaudio.functional.forced_align()\n[\u591a\u8bed\u8a00\u5bf9\u9f50](https://pytorch.org/audio/stable/tutorials/forced_alignment_for_multilingual_data_tutorial.html)\n\n\n### \u6ce2\u5f62\u5408\u6210\n\n- [\u632f\u8361\u6ce2](https://pytorch.org/audio/stable/tutorials/oscillator_tutorial.html)\n- [\u6ce2](https://pytorch.org/audio/stable/tutorials/additive_synthesis_tutorial.html)\n### [\u6ee4\u6ce2\u5668\u8bbe\u8ba1](https://pytorch.org/audio/stable/tutorials/filter_design_tutorial.html)\n\n\n\n\n### Piplines\n\n#### [\u57fa\u4e8eCTC\u8bed\u8a00\u8bc6\u522b](https://pytorch.org/audio/stable/tutorials/asr_inference_with_ctc_decoder_tutorial.html)\uff1a\n#### [\u5728\u7ebf\u7684\u57fa\u4e8eRNN-T\u7684ASR](https://pytorch.org/audio/stable/tutorials/online_asr_tutorial.html)\n#### [\u57fa\u4e8e\u9ea6\u514b\u98ce\u7684\u6d41\u5f0f\u8bc6\u522b](https://pytorch.org/audio/stable/tutorials/device_asr.html)\n\n\n### [\u6ce2\u675f\u6210\u5f62](https://pytorch.org/audio/stable/tutorials/mvdr_tutorial.html)\n\n```python\nimport torch\nimport torchaudio\nimport torchaudio.transforms as transforms\n\n# 1. \u52a0\u8f7d\u591a\u901a\u9053\u97f3\u9891\naudio_path = 'path_to_your_audio_file.wav'  # \u66ff\u6362\u4e3a\u4f60\u7684\u97f3\u9891\u8def\u5f84\nwaveform, sample_rate = torchaudio.load(audio_path)\nspecgram = torchaudio.transforms.Spectrogram(n_fft=512, hop_length=256)(waveform)\n\n# 2. \u8ba1\u7b97 PSD \u77e9\u9635\npsd = transforms.PSD()(specgram)\n\n# 3. \u5b9a\u4e49\u53c2\u8003\u901a\u9053\nreference_channel = 0\n\n# 4. \u4f7f\u7528 SoudenMVDR \u8fdb\u884c\u6ce2\u675f\u5f62\u6210\nmvdr = transforms.SoudenMVDR(ref_channel=reference_channel)\nenhanced_specgram = mvdr(specgram, psd, psd, reference_channel=reference_channel)\n\n# 5. \u5c06\u589e\u5f3a\u540e\u7684\u9891\u8c31\u8f6c\u6362\u56de\u65f6\u57df\u4fe1\u53f7\nenhanced_waveform = torchaudio.transforms.InverseSpectrogram(n_fft=512, hop_length=256)(enhanced_specgram)\n\n# 6. \u4fdd\u5b58\u589e\u5f3a\u540e\u7684\u97f3\u9891\ntorchaudio.save('enhanced_output.wav', enhanced_waveform, sample_rate)\n```\n\n```python\nimport torch\nimport torchaudio\nimport torchaudio.transforms as transforms\nimport torchaudio.functional as functional\n\n# 1. \u52a0\u8f7d\u591a\u901a\u9053\u97f3\u9891\naudio_path = 'path_to_your_audio_file.wav'  # \u66ff\u6362\u4e3a\u4f60\u7684\u97f3\u9891\u8def\u5f84\nwaveform, sample_rate = torchaudio.load(audio_path)\nspecgram = torchaudio.transforms.Spectrogram(n_fft=512, hop_length=256)(waveform)\n\n# 2. \u8ba1\u7b97 RTF \u5411\u91cf\nrtf = functional.rtf_evd(specgram, reference_channel=0)\n\n# 3. \u8ba1\u7b97\u566a\u58f0\u7684 PSD \u77e9\u9635\npsd_n = functional.psd(specgram, mask=noise_mask)\n\n# 4. \u5b9a\u4e49 RTFMVDR \u6a21\u5757\nrtf_mvdr = transforms.RTFMVDR(reference_channel=0)\n\n# 5. \u5e94\u7528\u6ce2\u675f\u5f62\u6210\nenhanced_specgram = rtf_mvdr(specgram, rtf, psd_n, reference_channel=0)\n\n# 6. \u5c06\u589e\u5f3a\u540e\u7684\u9891\u8c31\u8f6c\u6362\u56de\u65f6\u57df\u4fe1\u53f7\nenhanced_waveform = torchaudio.transforms.InverseSpectrogram(n_fft=512, hop_length=256)(enhanced_specgram)\n\n# 7. \u4fdd\u5b58\u589e\u5f3a\u540e\u7684\u97f3\u9891\ntorchaudio.save('rtf_mvdr_output.wav', enhanced_waveform, sample_rate)\n```\u3002", "top": 0, "createdAt": 1739328400, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-02-12", "dateLabelColor": "#0969da"}, "P14": {"htmlDir": "docs/post/Pytorch-fen-bu-shi-xun-lian.html", "labels": ["QuickNote"], "postTitle": "Pytorch\u5206\u5e03\u5f0f\u8bad\u7ec3", "postUrl": "post/Pytorch-fen-bu-shi-xun-lian.html", "postSourceUrl": "https://github.com/teamtee/teamtee.github.io/issues/14", "commentNum": 0, "wordCount": 9893, "description": "[Pytorch\u5206\u5e03\u5f0f\u6587\u7ae0](https://zhuanlan.zhihu.com/p/178402798)-\u63a8\u8350\n# \u7b80\u4ecb\n\nPyTorch\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u5141\u8bb8\u5728\u591a\u4e2aGPU\u6216\u591a\u53f0\u673a\u5668\u4e0a\u5e76\u884c\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u901f\u5ea6\u548c\u6269\u5c55\u6027\u3002", "top": 0, "createdAt": 1739352415, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-02-12", "dateLabelColor": "#0969da"}, "P15": {"htmlDir": "docs/post/Qwen-Audio-jie-du-xi-lie- 2.-yuan-ma-yu-Pytorch.html", "labels": ["QuickNote"], "postTitle": "Qwen-Audio\u89e3\u8bfb\u7cfb\u5217 2.\u6e90\u7801\u4e0ePytorch", "postUrl": "post/Qwen-Audio-jie-du-xi-lie-%202.-yuan-ma-yu-Pytorch.html", "postSourceUrl": "https://github.com/teamtee/teamtee.github.io/issues/15", "commentNum": 0, "wordCount": 8175, "description": "\n## Auto\u7c7b\n[\u53c2\u8003](https://huggingface.co/docs/transformers/model_doc/auto)\ntransformers\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u7684\u81ea\u52a8\u51fd\u6570\u7c7b\uff0c\u4ed6\u4eec\u63d0\u4f9b\u4e86\u901a\u7528\u7684\u52a0\u8f7d\u65b9\u5f0f\n- AutoConfig\n- AutoTokenizer\n- AutoFeatureExtractor\n- AutoProcessor\n- AutoModel\n- AutoModelFor...\n\t- AutoModelForCausalLM\n\t- ...\n\u5176\u4e2d\u503c\u5f97\u4e00\u63d0\u7684\u662f\n- AutoModel\u548cAutoModelFor...\u7684\u533a\u522b\u662f\u662f\u5426\u5305\u542b\u8f93\u51fa\u5934\uff0c\u6bd4\u5982\u6700\u540e\u5c06\u8f93\u51fa\u8f6c\u5316\u4e3a\u8bcd\u8868\u5927\u5c0f\u6765\u8f93\u51fa\n```plaintext\n(lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n```\n- AutoModel\u548cAutoModelFor...\u7684\u52a0\u8f7d\u6a21\u578b\u4e24\u79cd\u65b9\u5f0f\n```\nmodel = AutoModel.from_pretrained(checkpoint) # \u52a0\u8f7d\u6743\u91cd\nmodel = AutoModel.from_config(config) # \u4e0d\u52a0\u8f7d\u6743\u91cd\n```\n\u5b9e\u9645\u4e0a\u6211\u4eec\u53ef\u80fd\u4f1a\u91c7\u7528\u66f4\u52a0\u5177\u4f53\u7684\u6a21\u578b\uff0c\u6bd4\u5982`Qwen2AudioForConditionalGeneration`\n\n## \u5b9e\u4f8b\u6a21\u578b\n### PreTrainedModel\n\nPreTrainedModel \u662f\u4e00\u4e2a\u62bd\u8c61\u7c7b\uff0c\u5b9a\u4e49\u4e86\u5fc5\u987b\u7684\u64cd\u4f5c\n\n```\nclass PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin, PeftAdapterMixin):\n```\n **`nn.Module`**\uff1a\n    - PyTorch \u7684\u57fa\u7840\u6a21\u5757\u7c7b\uff0c\u4e3a\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u672c\u7684\u795e\u7ecf\u7f51\u7edc\u529f\u80fd\uff0c\u4f8b\u5982\u53c2\u6570\u7ba1\u7406\u3001\u524d\u5411\u4f20\u64ad\u7b49\u3002", "top": 0, "createdAt": 1740208846, "style": "", "script": "", "head": "", "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "createdDate": "2025-02-22", "dateLabelColor": "#0969da"}}, "singeListJson": {}, "labelColorDict": {"NoteList": "#ffffff", "QuickNote": "#0075ca", "ToolShare": "#d876e3"}, "ogImage": "https://github.githubassets.com/favicons/favicon.svg", "prevUrl": "disabled", "nextUrl": "disabled"}