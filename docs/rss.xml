<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>teamtee</title><link>http://teamtee.top</link><description>这里是提姆提的新小屋，旧小屋可以去[这里](http://teamtee.top/teamtee/)</description><copyright>teamtee</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>http://teamtee.top</link></image><lastBuildDate>Wed, 12 Feb 2025 09:27:22 +0000</lastBuildDate><managingEditor>teamtee</managingEditor><ttl>60</ttl><webMaster>teamtee</webMaster><item><title>Pytorch分布式训练</title><link>http://teamtee.top/post/Pytorch-fen-bu-shi-xun-lian.html</link><description>[Pytorch分布式文章](https://zhuanlan.zhihu.com/p/178402798)-推荐
# 简介

PyTorch的分布式训练允许在多个GPU或多台机器上并行训练模型，显著提升训练速度和扩展性。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fen-bu-shi-xun-lian.html</guid><pubDate>Wed, 12 Feb 2025 09:26:55 +0000</pubDate></item><item><title>Pytorch库的使用torchaudio</title><link>http://teamtee.top/post/Pytorch-ku-de-shi-yong-torchaudio.html</link><description>
[官网教程](https://pytorch.org/audio/main/)
## 前言

torchaudio.transformers是Pytorch官方提供的有关音频处理的库

### 读写
加载与保存
- torchaudio.info(path)
- torchaudio.load(path)
- torchaudio.save(path,wavform,sample_rate)
### StreamReader &amp; StreamWriter(略)
可用于流式的读写，支持麦克风，网络读写
### transformers
```
import torchaudio
import torchaudio.functional as F
import torchaudio.transforms as T
```
### Resample
要将音频波形从一个freqeeuncy重新采样，您可以使用 torchaudio.transforms.Resample 或者 torchaudio.functional.resample() 
```
resampler = T.Resample(sample_rate, resample_rate, dtype=waveform.dtype)
resampled_waveform = resampler(waveform)
```

```
resampled_waveform = F.resample(waveform, sample_rate, resample_rate, lowpass_filter_width=6)
```
### [Data Augmentation](https://pytorch.org/audio/stable/tutorials/audio_data_augmentation_tutorial.html)

####  filters滤波器

```python
# Define effects
effect = ','.join(
    [
        'lowpass=frequency=300:poles=1',  # apply single-pole lowpass filter
        'atempo=0.8',  # reduce the speed
        'aecho=in_gain=0.8:out_gain=0.9:delays=200:decays=0.3|delays=400:decays=0.3'
        # Applying echo gives some dramatic feeling
    ],
)
effector = torchaudio.io.AudioEffector(effect=effect)
effector.apply(waveform, sample_rate)
```

#### RIR混响模拟
![[rir.wav]]
```python
rir_raw, sample_rate = torchaudio.load(SAMPLE_RIR)
rir = rir_raw[:, int(sample_rate * 1.01) : int(sample_rate * 1.3)]
rir = rir / torch.linalg.vector_norm(rir, ord=2)
speech, _ = torchaudio.load(SAMPLE_SPEECH)
augmented = F.fftconvolve(speech, rir)
```
#### 添加噪声
```python
speech, _ = torchaudio.load(SAMPLE_SPEECH)
noise, _ = torchaudio.load(SAMPLE_NOISE)
noise = noise[:, : speech.shape[1]]

snr_dbs = torch.tensor([20, 10, 3])
noisy_speeches = F.add_noise(speech, noise, snr_dbs)
```

```python
import torch

import torchaudio

import torchaudio.functional as F

import random

  

# 加载语音和噪声文件

speech, _ = torchaudio.load('/hpc_stor01/home/yangui.fang_sx/workingspace/tools/R8009_M8024-8076-000350_000545.flac')

noise, _ = torchaudio.load('/hpc_stor01/home/yangui.fang_sx/workingspace/tools/rir.wav')

# 确保语音和噪声的采样率相同

assert fs == noise.shape[1], '语音和噪声的采样率必须相同'

  

# 如果语音比噪声长，随机选择噪声的起始点

if speech.shape[1] &gt; noise.shape[1]:

    # 随机选择噪声的起始点

    start_idx = random.randint(0, speech.shape[1] - noise.shape[1])

    # 在语音的随机位置开始添加噪声

    speech_with_noise = speech.clone()

    speech_with_noise[:, start_idx:start_idx + noise.shape[1]] += noise

else:

    # 如果噪声比语音长，从噪声的随机位置开始截取

    start_idx = random.randint(0, noise.shape[1] - speech.shape[1])

    noise = noise[:, start_idx:start_idx + speech.shape[1]]

    # 直接将噪声添加到语音中

    snr_dbs = random.randomint(1, 30)

    noisy_speeches = F.add_noise(speech, noise, snr_dbs)

  

# 保存带噪语音信号

Audio(speech_with_noise, rate=fs)

# output_path = 'noisy_speech.wav'

# torchaudio.save(output_path, speech_with_noise, fs)

# print(f'Saved noisy speech to {output_path}')
```
#### 编码

```python
encoder = torchaudio.io.AudioEffector(format=format, encoder=encoder)
encoder.apply(waveform, sample_rate)	
```
 format,encoder支持下面的，
- 'wav','pcm_mulaw'
- 'g722'
- 'ogg', encoder='vorbis'

[### Feature Extract 特征提取](https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html)

#### Spectrogram

```python
spectrogram = T.Spectrogram(n_fft=512)

# Perform transform
spec = spectrogram(wav)
```
#### GriffinLim 
基于规则的从频谱恢复波形

```python
# Define transforms
n_fft = 1024
spectrogram = T.Spectrogram(n_fft=n_fft)
griffin_lim = T.GriffinLim(n_fft=n_fft)

# Apply the transforms
spec = spectrogram(SPEECH_WAVEFORM)
reconstructed_waveform = griffin_lim(spec)
```
#### Melspectrogram

```python
n_fft = 1024
win_length = None
hop_length = 512
n_mels = 128

mel_spectrogram = T.MelSpectrogram(
    sample_rate=sample_rate,
    n_fft=n_fft,
    win_length=win_length,
    hop_length=hop_length,
    center=True,
    pad_mode='reflect',
    power=2.0,
    norm='slaney',
    n_mels=n_mels,
    mel_scale='htk',
)

melspec = mel_spectrogram(SPEECH_WAVEFORM)
```

#### MFCC

```python
n_fft = 2048
win_length = None
hop_length = 512
n_mels = 256
n_mfcc = 256

mfcc_transform = T.MFCC(
    sample_rate=sample_rate,
    n_mfcc=n_mfcc,
    melkwargs={
        'n_fft': n_fft,
        'n_mels': n_mels,
        'hop_length': hop_length,
        'mel_scale': 'htk',
    },
)

mfcc = mfcc_transform(SPEECH_WAVEFORM)
```
#### LFCC
```python
n_fft = 2048
win_length = None
hop_length = 512
n_lfcc = 256

lfcc_transform = T.LFCC(
    sample_rate=sample_rate,
    n_lfcc=n_lfcc,
    speckwargs={
        'n_fft': n_fft,
        'win_length': win_length,
        'hop_length': hop_length,
    },
)

lfcc = lfcc_transform(SPEECH_WAVEFORM)
plot_spectrogram(lfcc[0], title='LFCC')
```

#### Picth 音高

```python
pitch = F.detect_pitch_frequency(SPEECH_WAVEFORM, SAMPLE_RATE)
```



#### SpecAugment

```python

spec = spec.permute(1, 0).unsqueeze(0)
stretch = T.Spectrogram(n_freq=通道数)
rate = random.random()*0.2 + 0.9
spec = stretch(spec, rate)

Timemasking = T.TimeMasking(time_mask_param=100)
Frequencymasking = T.FrequencyMasking(freq_mask_param=27)
spec = Timemasking(spec)
spec = Timemasking(spec)
spec = Frequencymasking(spec)
spec = Frequencymasking(spec)  
```

### [CTC强制对齐](https://pytorch.org/audio/stable/tutorials/ctc_forced_alignment_api_tutorial.html)

- torchaudio.functional.forced_align()
[多语言对齐](https://pytorch.org/audio/stable/tutorials/forced_alignment_for_multilingual_data_tutorial.html)


### 波形合成

- [振荡波](https://pytorch.org/audio/stable/tutorials/oscillator_tutorial.html)
- [波](https://pytorch.org/audio/stable/tutorials/additive_synthesis_tutorial.html)
### [滤波器设计](https://pytorch.org/audio/stable/tutorials/filter_design_tutorial.html)




### Piplines

#### [基于CTC语言识别](https://pytorch.org/audio/stable/tutorials/asr_inference_with_ctc_decoder_tutorial.html)：
#### [在线的基于RNN-T的ASR](https://pytorch.org/audio/stable/tutorials/online_asr_tutorial.html)
#### [基于麦克风的流式识别](https://pytorch.org/audio/stable/tutorials/device_asr.html)


### [波束成形](https://pytorch.org/audio/stable/tutorials/mvdr_tutorial.html)

```python
import torch
import torchaudio
import torchaudio.transforms as transforms

# 1. 加载多通道音频
audio_path = 'path_to_your_audio_file.wav'  # 替换为你的音频路径
waveform, sample_rate = torchaudio.load(audio_path)
specgram = torchaudio.transforms.Spectrogram(n_fft=512, hop_length=256)(waveform)

# 2. 计算 PSD 矩阵
psd = transforms.PSD()(specgram)

# 3. 定义参考通道
reference_channel = 0

# 4. 使用 SoudenMVDR 进行波束形成
mvdr = transforms.SoudenMVDR(ref_channel=reference_channel)
enhanced_specgram = mvdr(specgram, psd, psd, reference_channel=reference_channel)

# 5. 将增强后的频谱转换回时域信号
enhanced_waveform = torchaudio.transforms.InverseSpectrogram(n_fft=512, hop_length=256)(enhanced_specgram)

# 6. 保存增强后的音频
torchaudio.save('enhanced_output.wav', enhanced_waveform, sample_rate)
```

```python
import torch
import torchaudio
import torchaudio.transforms as transforms
import torchaudio.functional as functional

# 1. 加载多通道音频
audio_path = 'path_to_your_audio_file.wav'  # 替换为你的音频路径
waveform, sample_rate = torchaudio.load(audio_path)
specgram = torchaudio.transforms.Spectrogram(n_fft=512, hop_length=256)(waveform)

# 2. 计算 RTF 向量
rtf = functional.rtf_evd(specgram, reference_channel=0)

# 3. 计算噪声的 PSD 矩阵
psd_n = functional.psd(specgram, mask=noise_mask)

# 4. 定义 RTFMVDR 模块
rtf_mvdr = transforms.RTFMVDR(reference_channel=0)

# 5. 应用波束形成
enhanced_specgram = rtf_mvdr(specgram, rtf, psd_n, reference_channel=0)

# 6. 将增强后的频谱转换回时域信号
enhanced_waveform = torchaudio.transforms.InverseSpectrogram(n_fft=512, hop_length=256)(enhanced_specgram)

# 7. 保存增强后的音频
torchaudio.save('rtf_mvdr_output.wav', enhanced_waveform, sample_rate)
```。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-ku-de-shi-yong-torchaudio.html</guid><pubDate>Wed, 12 Feb 2025 02:46:40 +0000</pubDate></item><item><title>Qwen-Audio解读系列</title><link>http://teamtee.top/post/Qwen-Audio-jie-du-xi-lie.html</link><description># 简介

Qwen-Audio是由阿里的Qwen团队开发的语音多模态大模型，分为Qwen-Audio，Qwen2-Audio,两者均具备强大的听觉能力，具备处理语音、音频事件和音乐歌曲的广泛能力

Paper：[Qwen-Audio](https://arxiv.org/abs/2311.07919)
Github：[Qwen-Audio](https://github.com/QwenLM/Qwen-Audio)
Paper：[Qwen2-Audio](https://arxiv.org/abs/2407.10759)
Github：[Qwen2-Audio](https://github.com/QwenLM/Qwen2-Audio)
（注释：Qwen-Audio的代码比Qwen2-Audio详细很多）
## 数据

Qwen

![Image](https://github.com/user-attachments/assets/2d2e32e9-ff90-45b4-a828-8665cfbd4e71)

- Speech：约89.2k(不足1k按照1k的计算)
- Sound：约16.8k
- Music：约40.5k

Qwen2-Audio
- Spech：37k
- Sound：10k
- Music：140k
## 模型
Qwen-Audio：Qwen-Audio的实际结构为 Whisper-large-v2+线性层组合+Qwen-7B(具体型号没找到)

线性层组合 = Linear3 * （（linear2* speech_tokens） * F.silu(linear1* speech_tokens)）
![Image](https://github.com/user-attachments/assets/bc90e9ea-03b4-4169-8e5c-2ceb00658cc1)

Qwen2-Audio:实际结构为Whisperlarge-v3+平均池化层(长2)+线性层+Qwen-7B(具体型号没找到)
![Image](https://github.com/user-attachments/assets/6319993a-5269-4a2b-82cc-00ed24b690e7)
## 训练
Qwen-audio:Qwen-Audio采用的是类似Whisper的训练框架，即为预测Token，并且Qwen-Audio只训练Whisper+MLP，不对语言模型进行微调，而且只是采用预训练，SFT得到的是Qwen-Audio-Chat

Qwen2-audio:采用Pretrain+SFT+DPO，具体训练的模型哪部分没提到
- Pretrain：训练ASR+ACC
- SFT：分为两类任务，语音分析任务：文本指令+语音，聊天任务：语音
- DPO：优化表现
## 结果
Qwen-Audio

![Image](https://github.com/user-attachments/assets/781b1d46-34a6-46fb-ab4e-bdc066b3a0fd)

Qwen2-Audio

![Image](https://github.com/user-attachments/assets/ee9a8c6c-42c7-4e6c-a6f9-abff8216c6b1)。</description><guid isPermaLink="true">http://teamtee.top/post/Qwen-Audio-jie-du-xi-lie.html</guid><pubDate>Tue, 11 Feb 2025 09:21:53 +0000</pubDate></item><item><title>Linux下常用好用命令总结</title><link>http://teamtee.top/post/Linux-xia-chang-yong-hao-yong-ming-ling-zong-jie.html</link><description>命令行的参数繁多，但是我们常常使用的不过寥寥，因此我将常用的命令行用法罗列如下&#13;
## Linux&#13;
## find&#13;
&#13;
```bash&#13;
find . -name '*ext' -o -name '*pattern*'&#13;
```&#13;
- -o表示加上另一个查找项目&#13;
## sed&#13;
```bash&#13;
sed -i 's::g' [filename]&#13;
```&#13;
- -i表示直接修改原文件不输出&#13;
## paste&#13;
按照行拼接两个文件&#13;
```bash&#13;
paste -d '' file1 file2&#13;
```&#13;
- -d 指定分割符&#13;
## split&#13;
按照行数细分文件&#13;
```bash&#13;
split -n 1000 file&#13;
split -n 100 -d file &#13;
```&#13;
- -d表示通过数字命令子文件，默认用字母&#13;
&#13;
## Python&#13;
&#13;
### torch&#13;
- python -m torch.utils.collect_env&#13;
收集形成详细的环境信息&#13;
。</description><guid isPermaLink="true">http://teamtee.top/post/Linux-xia-chang-yong-hao-yong-ming-ling-zong-jie.html</guid><pubDate>Fri, 03 Jan 2025 08:35:44 +0000</pubDate></item><item><title>正则表达式</title><link>http://teamtee.top/post/zheng-ze-biao-da-shi.html</link><description># 简介&#13;
正则表达式是一门在发展中逐渐形成的学问，因此存在历史版本的兼容性问题，有很多规范，现在我们使用的规范基本都和POSIX的规范一致。</description><guid isPermaLink="true">http://teamtee.top/post/zheng-ze-biao-da-shi.html</guid><pubDate>Fri, 03 Jan 2025 08:31:47 +0000</pubDate></item><item><title>Pytorch复习系列6:torch.ddp+fsdp</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-6-torch.ddp%2Bfsdp.html</link><description>[分布式深度学习训练中DP,DDP,FSDP这三者之间的区别和联系是什么](https://blog.csdn.net/Flemington7/article/details/139031199)&#13;
[Pytorch_DDP](https://pytorch.org/tutorials/beginner/ddp_series_theory.html)&#13;
[PYtorch_DDP_start](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)&#13;
[Pytorch_DP](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)&#13;
[Pytorch_FSDP](https://pytorch.ac.cn/tutorials/intermediate/FSDP_tutorial.html)&#13;
[DDP的好的上手教程](https://www.cnblogs.com/gzyatcnblogs/articles/17946484)&#13;
[DeepSpeed官网](https://docs.deepspeed.org.cn/en/latest/activation-checkpointing.html)&#13;
[DeepSpeed上手好教程](https://www.tutorialspoint.com/deepspeed/index.htm)&#13;
[有关Reduce\Gather\Scatter的概念文章](https://cloud.tencent.com/developer/article/2306663)&#13;
[DDP深度解析好文](https://zhuanlan.zhihu.com/p/178402798)&#13;
[有关FSDP内存消耗的绝世好文章](https://cloud.tencent.com/developer/article/2314837)&#13;
## 原理&#13;
&#13;
有关分布式思想有两个概念：&#13;
- DP：数据并行&#13;
- MP：模型并行&#13;
有关分布式的实践有三个概念&#13;
- DP：数据并行(数据并行)&#13;
- DDP：分布式数据并行(数据并行)&#13;
- FSDP：完全共享式数据并行(数据并行+模型并行)&#13;
有关分布式模型并行的论文产生了几个概念：&#13;
- Zero0：不分片&#13;
- ZeRO1：只把优化器状态进行分片&#13;
- ZeRO2：对优化器状态 + 梯度进行分片&#13;
- ZeRO3：对优化器状态 + 梯度 + 模型参数进行分片&#13;
除此之外还有些概念&#13;
- 流水线并行：pipline&#13;
- 激活检查点：Activation checkpoint&#13;
- 模型卸载：model offload&#13;
&#13;
## DDP&#13;
&#13;
DDP和DP的区别是，DP采用中心服务器来更新模型参数、收集梯度、分发新模型，DDP则完全采用分布式的做法，采用Ring-Reduce来同步梯度。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-6-torch.ddp%2Bfsdp.html</guid><pubDate>Tue, 31 Dec 2024 07:15:53 +0000</pubDate></item><item><title>Pytorch复习系列5:torch.cuda.amp</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-5-torch.cuda.amp.html</link><description># 前言&#13;
&#13;
混合精度训练的核心观点：**采用更低精度的类型进行运算会使用更少的内存和更快的速度**&#13;
必须采用Tensor core的核心显卡： GPU 中的 Tensor Core 天然支持 FP16 乘积的结果与 FP32 的累加&#13;
## 原理&#13;
[Mixed Precision Training](https://arxiv.org/abs/1710.03740)&#13;
&#13;
[有关参数的讲解的好文章](https://www.53ai.com/news/finetuning/2024083051493.html)&#13;
[有关torch.cuda.amp的好文章](https://zhuanlan.zhihu.com/p/348554267)&#13;
[讲解DeepSpeed的好文章](https://basicv8vc.github.io/posts/zero/)&#13;
[有关FSDP内存消耗的绝世好文章](https://cloud.tencent.com/developer/article/2314837)&#13;
## 参数类型&#13;
模型在保存的时候通常有下面四种类型&#13;
- fp32&#13;
- tf32&#13;
- fp16&#13;
- bf16&#13;
![image](https://github.com/user-attachments/assets/2211ccf2-b62a-4de8-8eac-4fbda5d599cc)&#13;
我们需要区分下面的概念，保存类型通常时预训练模型已经指定好的，加载类型我们可以指定，在运算时模型会自动将将运算的类型转换为模型的加载类型&#13;
&#13;
- 保存类型：&#13;
- 加载类型：&#13;
- 运算类型:&#13;
指定加载类型&#13;
```python&#13;
from transformers import AutoModel&#13;
# 加载模型时指定参数类型为float16&#13;
model = AutoModel.from_pretrained('bert-base-uncased', torch_dtype=torch.float16)&#13;
# 模型运算时，如果使用GPU，会自动使用对应的参数类型进行计算&#13;
# 例如，在NVIDIA GPU上，float16运算会使用Tensor Cores加速&#13;
```&#13;
指定加载类型，并且量化&#13;
```python&#13;
from transformers import AutoModel&#13;
from bitsandbytes as bnb&#13;
&#13;
# 指定量化配置&#13;
量化配置 = bnb.QuantizationConfig(&#13;
    load_in_8bit=True,&#13;
    bnb_8bit_quant_type='nf4',&#13;
    bnb_8bit_use_double_quant=False,&#13;
)&#13;
&#13;
# 加载并量化模型&#13;
model = AutoModel.from_pretrained(&#13;
    'bert-base-uncased',&#13;
    quantization_config=量化配置,&#13;
)&#13;
```&#13;
混合精度运算的核心思想：采用较高精度的参数类型加载模型，但是运算时将一些运算转化为低精度的参数类型来加快**训练和运算**，具体转化什么算子由pytorch自动决定。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-5-torch.cuda.amp.html</guid><pubDate>Tue, 31 Dec 2024 06:33:32 +0000</pubDate></item><item><title>Pytorch复习系列4:nn.Modules模块</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-4-nn.Modules-mo-kuai.html</link><description># 前言&#13;
nn.Modules下面包含很多nn.Module的实例，nn.Module是Pytorch所有神经网络的父类&#13;
参考&#13;
[1.PyTorch 源码解读之 nn.Module：核心网络模块接口详解](https://zhuanlan.zhihu.com/p/340453841)&#13;
&#13;
# nn.Module基本属性&#13;
在Module的__init__函数中可能观察到下面nn.Modules的核心组件&#13;
&#13;
```python&#13;
self.training = True  # 控制 training/testing 状态&#13;
self._parameters = OrderedDict()  # 在训练过程中会随着 BP 而更新的参数&#13;
self._buffers = OrderedDict()  # 在训练过程中不会随着 BP 而更新的参数&#13;
self._non_persistent_buffers_set = set()&#13;
self._backward_hooks = OrderedDict()  # Backward 完成后会被调用的 hook&#13;
self._forward_hooks = OrderedDict()  # Forward 完成后会被调用的 hook&#13;
self._forward_pre_hooks = OrderedDict()  # Forward 前会被调用的 hook&#13;
self._state_dict_hooks = OrderedDict()  # 得到 state_dict 以后会被调用的 hook&#13;
self._load_state_dict_pre_hooks = OrderedDict()  # load state_dict 前会被调用的 hook&#13;
self._modules = OrderedDict()  # 子神经网络模块&#13;
```&#13;
&#13;
## 基本属性&#13;
下面的函数可以获取这些参数&#13;
- named_parameters：返回自身parameters,如果 recurse=True 还会返回子模块中的模型参数&#13;
- named_buffers：返回自身parameters,如果 recurse=True 还会返回子模块中的模型 buffer&#13;
- named_children：返回自身的Modules&#13;
-  named_modules：返回自身和子Modules的Moduels(递归调用)&#13;
&#13;
下面的参数是对上面的调用,默认recurse参数为True&#13;
- parameters：&#13;
-  buffers：&#13;
-  children：&#13;
-  modules：&#13;
添加参数&#13;
- add_module：增加子神经网络模块，更新 self._modules&#13;
```&#13;
add_module(name,module)&#13;
```&#13;
-  register_parameter：增加通过 BP 可以更新的 parameters （如 BN 和 Conv 中的 weight 和 bias ），更新 self._parameters&#13;
- register_buffer：增加不通过 BP 更新的 buffer（如 BN 中的 running_mean 和 running_var）&#13;
- self.xxx = xxx ：该方法不会被登记，不属于Paramets和buffer，进行状态转换的时候会被遗漏&#13;
下面的函数可以调整梯度&#13;
- train()&#13;
- eval()&#13;
- requires_grad_()&#13;
- zero_gred()&#13;
&#13;
下面的函数可以映射parameters和buffers&#13;
- `_apply(fn)`:针对parameters和buffers通过调用所有parameters和buffers的tensor的_apply函数实现&#13;
&#13;
```&#13;
1. CPU：将所有 parameters 和 buffer 转移到 CPU 上&#13;
2. type：将所有 parameters 和 buffer 转变成另一个类型&#13;
3. CUDA：将所有 parameters 和 buffer 转移到 GPU 上&#13;
4. float：将所有浮点类型的 parameters 和 buffer 转变成 float32&#13;
5. double：将所有浮点类型的 parameters 和 buffer 转变成 double 类型&#13;
6. half：将所有浮点类型的 parameters 和 buffer 转变成 float16 类型&#13;
8. to：移动模块或/和改变模块的类型&#13;
```&#13;
- `apply`:针对Moduels，&#13;
可以自定义一个 init_weights 函数，通过 `net.apply(init_weights)` 来初始化模型权重。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-4-nn.Modules-mo-kuai.html</guid><pubDate>Tue, 24 Dec 2024 06:54:20 +0000</pubDate></item><item><title>Pytorch复习系列3:nn.Parameters参数</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-3-nn.Parameters-can-shu.html</link><description># 前言&#13;
&#13;
Parameter和Buffer都是实例化的Tensor，Parameter是参与梯度运算的参数，Buffer是不参与梯度计算的参数&#13;
&#13;
`class Parameter(torch.Tensor, metaclass=_ParameterMeta):`&#13;
&#13;
- `Parameter` 是一个特殊的张量，它被用来表示模型的参数,自动将 `Parameter` &#13;
&#13;
 `class Buffer(torch.Tensor, metaclass=_BufferMeta):`&#13;
 &#13;
- `Buffer` 也是一个特殊的张量，它用于存储那些在模型中不直接参与梯度计算的数据，但可能在模型的前向或后向传播中使用。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-3-nn.Parameters-can-shu.html</guid><pubDate>Tue, 24 Dec 2024 02:52:34 +0000</pubDate></item><item><title>Pytorch复习系列2:Dataset数据集</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-2-Dataset-shu-ju-ji.html</link><description># 前言&#13;
&#13;
Dataset是存储数据的集合，。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-2-Dataset-shu-ju-ji.html</guid><pubDate>Sat, 21 Dec 2024 14:16:18 +0000</pubDate></item><item><title>Pytorch复习系列1:Tensor张量</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-1-Tensor-zhang-liang.html</link><description># 前言&#13;
&#13;
Pytorch计算的基本单位就是Tensor,中文名张量。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-1-Tensor-zhang-liang.html</guid><pubDate>Sat, 21 Dec 2024 11:40:47 +0000</pubDate></item><item><title>Pytorch复习系列0:卷首语</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-0--juan-shou-yu.html</link><description># 前言&#13;
&#13;
在动笔写下这篇系列的第一篇博客开始，我就必须要提醒自己，为什么要写下《Pytorch入门》的博客，市面上不是有很多笔记和教程了吗，甚至你自己都是通过这些资料来入门Pytorch的，还需要你写入门教程吗？&#13;
&#13;
确实是的，我想市面上的资料已经很全了，但是我觉得还有一些不足：&#13;
- 缺乏系统性：系统性指的是两个方面，知识的结构性和层次性，市面上的资料往往是分散的，缺乏从一个系统的角度来阐明要义，总是局限于某一种应用，并且知识往往不具备层次性，要么过深，要么过浅，要么过度的难度过于陡峭。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-0--juan-shou-yu.html</guid><pubDate>Sat, 21 Dec 2024 11:38:45 +0000</pubDate></item><item><title>一文搞懂Whisper系列1:使用Whisper进行识别和特征提取</title><link>http://teamtee.top/post/yi-wen-gao-dong-Whisper-xi-lie-1--shi-yong-Whisper-jin-xing-shi-bie-he-te-zheng-ti-qu.html</link><description>&#13;
&#13;
[Whisper的PT文件下载地址](https://gitcode.csdn.net/65ed73ad1a836825ed799909.html)&#13;
[在Colab微调Whisper](https://huggingface.co/blog/fine-tune-whisper)&#13;
&#13;
## Whisper简介&#13;
&#13;
Whisper是Openai开发的语音识别工具，通常我们可以用Whisper库或者Transformers来使用Whisper，本文专注于Whisper库的使用，安装方式如下&#13;
&#13;
```python&#13;
pip install -U openai-whisper&#13;
```&#13;
&#13;
还需要安装ffmpeg&#13;
```&#13;
conda install ffmpeg(支持非sudo用户)&#13;
sudo apt install ffmpeg &#13;
```&#13;
&#13;
Whisper包含encoder和decoder两个部分,encoder接受30s的音频长度的输出，编码成为特征向量，decoder负责解码&#13;
&#13;
&#13;
## Whisper识别&#13;
`transcribe`:&#13;
- 最简单的识别方式&#13;
```python&#13;
import whisper&#13;
model = whisper.load('path/name')&#13;
text = model.transcribe('wav_path')&#13;
&#13;
```&#13;
`decode`：&#13;
- 注意到音频会被`pad_or_trim`函数填充或者裁剪为30s长度&#13;
- `decode`支持`mel`输入或者`encoder`编码后的特征输入&#13;
- `nmels=80/128`,128适合v3，80适合其他版本&#13;
```python&#13;
import whisper&#13;
import numpy as np&#13;
model = whisper.load_model('')&#13;
audio = whisper.load_audio('')&#13;
audio = whisper.pad_or_trim(audio)&#13;
mel = whisper.log_mel_spectrogram(audio,n_mels=model.dims.n_mels).to('cuda').to(model.device)&#13;
result = model.decode(mel)&#13;
# result = whisper.decode(model, mel)&#13;
&#13;
&#13;
```&#13;
&#13;
```python&#13;
import whisper&#13;
import numpy as np&#13;
model = whisper.load_model('')&#13;
audio = whisper.load_audio('')&#13;
audio = whisper.pad_or_trim(audio)&#13;
mel = whisper.log_mel_spectrogram(audio,n_mels=model.dims.n_mels).to('cuda').to(model.device)&#13;
result =model.decode(mel)&#13;
encoder_output = model.encoder(mel.unsqueeze(0))&#13;
&#13;
result = model.decode(encoder_output)&#13;
# result = whisper.decode(model, encoder_output)&#13;
# 打印encoder输出的形状&#13;
```&#13;
## Whisper提取特征&#13;
如果采用whisper的encoder提取特征，音频首先要被填充到30s&#13;
```python&#13;
import whisper&#13;
import numpy as np&#13;
model = whisper.load_model('')&#13;
audio = whisper.load_audio('')&#13;
audio = whisper.pad_or_trim(audio)&#13;
mel = whisper.log_mel_spectrogram(audio,n_mels=model.dims.n_mels).to('cuda').to(model.device)&#13;
result =model.decode(mel)&#13;
encoder_output = model.encoder(mel.unsqueeze(0))&#13;
&#13;
```&#13;
&#13;
可以采用替代forward函数的方法来提取不定长度的特征,因为encoder不支持小于30s长度音频的原因在于&#13;
- `x = (x + self.positional_embedding).to(x.dtype)`&#13;
&#13;
```python&#13;
import types&#13;
import whisper&#13;
import torch&#13;
import torch.nn as nn&#13;
import torch.nn.functional as F&#13;
def whisper_encoder_forward_monkey_patch(self, x: torch.Tensor):&#13;
	'''&#13;
	x : torch.Tensor, shape = (batch_size, n_mels, n_ctx)&#13;
	the mel spectrogram of the audio&#13;
	'''&#13;
	x = F.gelu(self.conv1(x))&#13;
	x = F.gelu(self.conv2(x))&#13;
	x = x.permute(0, 2, 1)&#13;
	# assert x.shape[1:] == self.positional_embedding.shape, 'incorrect audio shape'&#13;
	# x = (x + self.positional_embedding).to(x.dtype)&#13;
	x = (x + self.positional_embedding[: x.shape[1]]).to(x.dtype)&#13;
	for block in self.blocks:&#13;
		x = block(x)&#13;
		x = self.ln_post(x)&#13;
	return x&#13;
```&#13;
&#13;
```python&#13;
&#13;
encoder = whisper.load_model('base').encoder&#13;
encoder.whisper_encoder_forward_monkey_patch = types.MethodType(whisper_encoder_forward_monkey_patch, encoder)&#13;
audio_path = ''&#13;
audio = whisper.load_audio(audio_path)&#13;
mel = whisper.log_mel_spectrogram(audio).to(model.device)&#13;
features = encoder.whisper_encoder_forward_monkey_patch(mel.unsqueeze(0))&#13;
```&#13;
&#13;
```python&#13;
whisper.model.AudioEncoder.forward = forward&#13;
model = whisper.load_model('')&#13;
audio = whisper.load_audio('')&#13;
mel = whisper.log_mel_spectrogram(audio,n_mels=model.dims.n_mels).to('cuda').to(model.device).unsquenze(0)&#13;
outout = model.encoder(mel)&#13;
```&#13;
如果需要whisper提取出的该特征进行解码，必须使用options&#13;
&#13;
```python&#13;
options = whisper.DecodingOptions(&#13;
    task='transcribe',&#13;
    language='zh',&#13;
    without_timestamps=True,&#13;
    beam_size=4,&#13;
&#13;
)&#13;
print(whisper.decode(model,mel,options))&#13;
```&#13;
&#13;
## Whisper Options&#13;
- `task`:默认为`transcribe`，可以设置为`translate`,即为将输出翻译为英语&#13;
&#13;
&#13;
&#13;
## Huggingface用法&#13;
```python&#13;
from transformers import WhisperForConditionalGeneration, WhisperProcessor&#13;
# 加载预训练的Whisper模型和处理器&#13;
model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-base')&#13;
processor = WhisperProcessor.from_pretrained('openai/whisper-base')&#13;
# 假设你有一个输入的语音特征&#13;
input_features = ...  # 这里应该是预处理后的语音特征&#13;
# 将输入特征移动到模型所在的设备上&#13;
input_features = input_features.to(model.device)&#13;
# 使用分块算法生成输出&#13;
outputs = model.generate(&#13;
    input_features=input_features,&#13;
    return_dict_in_generate=True,&#13;
    output_hidden_states=True,&#13;
    chunk_length=30,  # 设置分块长度&#13;
    stride_length=15  # 设置步长&#13;
)&#13;
# 解码生成的序列&#13;
transcriptions = processor.batch_decode(outputs.sequences, skip_special_tokens=True)[0]&#13;
# 打印转录结果&#13;
print(transcriptions)&#13;
&#13;
```。</description><guid isPermaLink="true">http://teamtee.top/post/yi-wen-gao-dong-Whisper-xi-lie-1--shi-yong-Whisper-jin-xing-shi-bie-he-te-zheng-ti-qu.html</guid><pubDate>Thu, 12 Dec 2024 08:54:05 +0000</pubDate></item></channel></rss>