<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>teamtee</title><link>http://teamtee.top</link><description>这里是提姆提的新小屋，旧小屋可以去[这里](http://teamtee.top/teamtee/)</description><copyright>teamtee</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>http://teamtee.top</link></image><lastBuildDate>Fri, 03 Jan 2025 08:59:25 +0000</lastBuildDate><managingEditor>teamtee</managingEditor><ttl>60</ttl><webMaster>teamtee</webMaster><item><title>Linux下常用好用命令总结</title><link>http://teamtee.top/post/Linux-xia-chang-yong-hao-yong-ming-ling-zong-jie.html</link><description>命令行的参数繁多，但是我们常常使用的不过寥寥，因此我将常用的命令行用法罗列如下&#13;
## Linux&#13;
## find&#13;
&#13;
```bash&#13;
find . -name '*ext' -o -name '*pattern*'&#13;
```&#13;
- -o表示加上另一个查找项目&#13;
## sed&#13;
```bash&#13;
sed -i 's::g' [filename]&#13;
```&#13;
- -i表示直接修改原文件不输出&#13;
## paste&#13;
按照行拼接两个文件&#13;
```bash&#13;
paste -d '' file1 file2&#13;
```&#13;
- -d 指定分割符&#13;
## split&#13;
按照行数细分文件&#13;
```bash&#13;
split -n 1000 file&#13;
split -n 100 -d file &#13;
```&#13;
- -d表示通过数字命令子文件，默认用字母&#13;
&#13;
## Python&#13;
&#13;
### torch&#13;
- python -m torch.utils.collect_env&#13;
收集形成详细的环境信息&#13;
。</description><guid isPermaLink="true">http://teamtee.top/post/Linux-xia-chang-yong-hao-yong-ming-ling-zong-jie.html</guid><pubDate>Fri, 03 Jan 2025 08:35:44 +0000</pubDate></item><item><title>正则表达式</title><link>http://teamtee.top/post/zheng-ze-biao-da-shi.html</link><description># 简介&#13;
正则表达式是一门在发展中逐渐形成的学问，因此存在历史版本的兼容性问题，有很多规范，现在我们使用的规范基本都和POSIX的规范一致。</description><guid isPermaLink="true">http://teamtee.top/post/zheng-ze-biao-da-shi.html</guid><pubDate>Fri, 03 Jan 2025 08:31:47 +0000</pubDate></item><item><title>Pytorch复习系列6:torch.ddp+fsdp</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-6-torch.ddp%2Bfsdp.html</link><description>[分布式深度学习训练中DP,DDP,FSDP这三者之间的区别和联系是什么](https://blog.csdn.net/Flemington7/article/details/139031199)&#13;
[Pytorch_DDP](https://pytorch.org/tutorials/beginner/ddp_series_theory.html)&#13;
[PYtorch_DDP_start](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)&#13;
[Pytorch_DP](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html)&#13;
[Pytorch_FSDP](https://pytorch.ac.cn/tutorials/intermediate/FSDP_tutorial.html)&#13;
[DDP的好的上手教程](https://www.cnblogs.com/gzyatcnblogs/articles/17946484)&#13;
[DeepSpeed官网](https://docs.deepspeed.org.cn/en/latest/activation-checkpointing.html)&#13;
[DeepSpeed上手好教程](https://www.tutorialspoint.com/deepspeed/index.htm)&#13;
[有关Reduce\Gather\Scatter的概念文章](https://cloud.tencent.com/developer/article/2306663)&#13;
[DDP深度解析好文](https://zhuanlan.zhihu.com/p/178402798)&#13;
[有关FSDP内存消耗的绝世好文章](https://cloud.tencent.com/developer/article/2314837)&#13;
## 原理&#13;
&#13;
有关分布式思想有两个概念：&#13;
- DP：数据并行&#13;
- MP：模型并行&#13;
有关分布式的实践有三个概念&#13;
- DP：数据并行(数据并行)&#13;
- DDP：分布式数据并行(数据并行)&#13;
- FSDP：完全共享式数据并行(数据并行+模型并行)&#13;
有关分布式模型并行的论文产生了几个概念：&#13;
- Zero0：不分片&#13;
- ZeRO1：只把优化器状态进行分片&#13;
- ZeRO2：对优化器状态 + 梯度进行分片&#13;
- ZeRO3：对优化器状态 + 梯度 + 模型参数进行分片&#13;
除此之外还有些概念&#13;
- 流水线并行：pipline&#13;
- 激活检查点：Activation checkpoint&#13;
- 模型卸载：model offload&#13;
&#13;
## DDP&#13;
&#13;
DDP和DP的区别是，DP采用中心服务器来更新模型参数、收集梯度、分发新模型，DDP则完全采用分布式的做法，采用Ring-Reduce来同步梯度。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-6-torch.ddp%2Bfsdp.html</guid><pubDate>Tue, 31 Dec 2024 07:15:53 +0000</pubDate></item><item><title>Pytorch复习系列5:torch.cuda.amp</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-5-torch.cuda.amp.html</link><description># 前言&#13;
&#13;
混合精度训练的核心观点：**采用更低精度的类型进行运算会使用更少的内存和更快的速度**&#13;
必须采用Tensor core的核心显卡： GPU 中的 Tensor Core 天然支持 FP16 乘积的结果与 FP32 的累加&#13;
## 原理&#13;
[Mixed Precision Training](https://arxiv.org/abs/1710.03740)&#13;
&#13;
[有关参数的讲解的好文章](https://www.53ai.com/news/finetuning/2024083051493.html)&#13;
[有关torch.cuda.amp的好文章](https://zhuanlan.zhihu.com/p/348554267)&#13;
[讲解DeepSpeed的好文章](https://basicv8vc.github.io/posts/zero/)&#13;
[有关FSDP内存消耗的绝世好文章](https://cloud.tencent.com/developer/article/2314837)&#13;
## 参数类型&#13;
模型在保存的时候通常有下面四种类型&#13;
- fp32&#13;
- tf32&#13;
- fp16&#13;
- bf16&#13;
![image](https://github.com/user-attachments/assets/2211ccf2-b62a-4de8-8eac-4fbda5d599cc)&#13;
我们需要区分下面的概念，保存类型通常时预训练模型已经指定好的，加载类型我们可以指定，在运算时模型会自动将将运算的类型转换为模型的加载类型&#13;
&#13;
- 保存类型：&#13;
- 加载类型：&#13;
- 运算类型:&#13;
指定加载类型&#13;
```python&#13;
from transformers import AutoModel&#13;
# 加载模型时指定参数类型为float16&#13;
model = AutoModel.from_pretrained('bert-base-uncased', torch_dtype=torch.float16)&#13;
# 模型运算时，如果使用GPU，会自动使用对应的参数类型进行计算&#13;
# 例如，在NVIDIA GPU上，float16运算会使用Tensor Cores加速&#13;
```&#13;
指定加载类型，并且量化&#13;
```python&#13;
from transformers import AutoModel&#13;
from bitsandbytes as bnb&#13;
&#13;
# 指定量化配置&#13;
量化配置 = bnb.QuantizationConfig(&#13;
    load_in_8bit=True,&#13;
    bnb_8bit_quant_type='nf4',&#13;
    bnb_8bit_use_double_quant=False,&#13;
)&#13;
&#13;
# 加载并量化模型&#13;
model = AutoModel.from_pretrained(&#13;
    'bert-base-uncased',&#13;
    quantization_config=量化配置,&#13;
)&#13;
```&#13;
混合精度运算的核心思想：采用较高精度的参数类型加载模型，但是运算时将一些运算转化为低精度的参数类型来加快**训练和运算**，具体转化什么算子由pytorch自动决定。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-5-torch.cuda.amp.html</guid><pubDate>Tue, 31 Dec 2024 06:33:32 +0000</pubDate></item><item><title>Pytorch复习系列4:nn.Modules模块</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-4-nn.Modules-mo-kuai.html</link><description># 前言&#13;
nn.Modules下面包含很多nn.Module的实例，nn.Module是Pytorch所有神经网络的父类&#13;
参考&#13;
[1.PyTorch 源码解读之 nn.Module：核心网络模块接口详解](https://zhuanlan.zhihu.com/p/340453841)&#13;
&#13;
# nn.Module基本属性&#13;
在Module的__init__函数中可能观察到下面nn.Modules的核心组件&#13;
&#13;
```python&#13;
self.training = True  # 控制 training/testing 状态&#13;
self._parameters = OrderedDict()  # 在训练过程中会随着 BP 而更新的参数&#13;
self._buffers = OrderedDict()  # 在训练过程中不会随着 BP 而更新的参数&#13;
self._non_persistent_buffers_set = set()&#13;
self._backward_hooks = OrderedDict()  # Backward 完成后会被调用的 hook&#13;
self._forward_hooks = OrderedDict()  # Forward 完成后会被调用的 hook&#13;
self._forward_pre_hooks = OrderedDict()  # Forward 前会被调用的 hook&#13;
self._state_dict_hooks = OrderedDict()  # 得到 state_dict 以后会被调用的 hook&#13;
self._load_state_dict_pre_hooks = OrderedDict()  # load state_dict 前会被调用的 hook&#13;
self._modules = OrderedDict()  # 子神经网络模块&#13;
```&#13;
&#13;
## 基本属性&#13;
下面的函数可以获取这些参数&#13;
- named_parameters：返回自身parameters,如果 recurse=True 还会返回子模块中的模型参数&#13;
- named_buffers：返回自身parameters,如果 recurse=True 还会返回子模块中的模型 buffer&#13;
- named_children：返回自身的Modules&#13;
-  named_modules：返回自身和子Modules的Moduels(递归调用)&#13;
&#13;
下面的参数是对上面的调用,默认recurse参数为True&#13;
- parameters：&#13;
-  buffers：&#13;
-  children：&#13;
-  modules：&#13;
添加参数&#13;
- add_module：增加子神经网络模块，更新 self._modules&#13;
```&#13;
add_module(name,module)&#13;
```&#13;
-  register_parameter：增加通过 BP 可以更新的 parameters （如 BN 和 Conv 中的 weight 和 bias ），更新 self._parameters&#13;
- register_buffer：增加不通过 BP 更新的 buffer（如 BN 中的 running_mean 和 running_var）&#13;
- self.xxx = xxx ：该方法不会被登记，不属于Paramets和buffer，进行状态转换的时候会被遗漏&#13;
下面的函数可以调整梯度&#13;
- train()&#13;
- eval()&#13;
- requires_grad_()&#13;
- zero_gred()&#13;
&#13;
下面的函数可以映射parameters和buffers&#13;
- `_apply(fn)`:针对parameters和buffers通过调用所有parameters和buffers的tensor的_apply函数实现&#13;
&#13;
```&#13;
1. CPU：将所有 parameters 和 buffer 转移到 CPU 上&#13;
2. type：将所有 parameters 和 buffer 转变成另一个类型&#13;
3. CUDA：将所有 parameters 和 buffer 转移到 GPU 上&#13;
4. float：将所有浮点类型的 parameters 和 buffer 转变成 float32&#13;
5. double：将所有浮点类型的 parameters 和 buffer 转变成 double 类型&#13;
6. half：将所有浮点类型的 parameters 和 buffer 转变成 float16 类型&#13;
8. to：移动模块或/和改变模块的类型&#13;
```&#13;
- `apply`:针对Moduels，&#13;
可以自定义一个 init_weights 函数，通过 `net.apply(init_weights)` 来初始化模型权重。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-4-nn.Modules-mo-kuai.html</guid><pubDate>Tue, 24 Dec 2024 06:54:20 +0000</pubDate></item><item><title>Pytorch复习系列3:nn.Parameters参数</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-3-nn.Parameters-can-shu.html</link><description># 前言&#13;
&#13;
Parameter和Buffer都是实例化的Tensor，Parameter是参与梯度运算的参数，Buffer是不参与梯度计算的参数&#13;
&#13;
`class Parameter(torch.Tensor, metaclass=_ParameterMeta):`&#13;
&#13;
- `Parameter` 是一个特殊的张量，它被用来表示模型的参数,自动将 `Parameter` &#13;
&#13;
 `class Buffer(torch.Tensor, metaclass=_BufferMeta):`&#13;
 &#13;
- `Buffer` 也是一个特殊的张量，它用于存储那些在模型中不直接参与梯度计算的数据，但可能在模型的前向或后向传播中使用。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-3-nn.Parameters-can-shu.html</guid><pubDate>Tue, 24 Dec 2024 02:52:34 +0000</pubDate></item><item><title>Pytorch复习系列2:Dataset数据集</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-2-Dataset-shu-ju-ji.html</link><description># 前言&#13;
&#13;
Dataset是存储数据的集合，。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-2-Dataset-shu-ju-ji.html</guid><pubDate>Sat, 21 Dec 2024 14:16:18 +0000</pubDate></item><item><title>Pytorch复习系列1:Tensor张量</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-1-Tensor-zhang-liang.html</link><description># 前言&#13;
&#13;
Pytorch计算的基本单位就是Tensor,中文名张量。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-1-Tensor-zhang-liang.html</guid><pubDate>Sat, 21 Dec 2024 11:40:47 +0000</pubDate></item><item><title>Pytorch复习系列0:卷首语</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-0--juan-shou-yu.html</link><description># 前言&#13;
&#13;
在动笔写下这篇系列的第一篇博客开始，我就必须要提醒自己，为什么要写下《Pytorch入门》的博客，市面上不是有很多笔记和教程了吗，甚至你自己都是通过这些资料来入门Pytorch的，还需要你写入门教程吗？&#13;
&#13;
确实是的，我想市面上的资料已经很全了，但是我觉得还有一些不足：&#13;
- 缺乏系统性：系统性指的是两个方面，知识的结构性和层次性，市面上的资料往往是分散的，缺乏从一个系统的角度来阐明要义，总是局限于某一种应用，并且知识往往不具备层次性，要么过深，要么过浅，要么过度的难度过于陡峭。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-0--juan-shou-yu.html</guid><pubDate>Sat, 21 Dec 2024 11:38:45 +0000</pubDate></item><item><title>一文搞懂Whisper系列1:使用Whisper进行识别和特征提取</title><link>http://teamtee.top/post/yi-wen-gao-dong-Whisper-xi-lie-1--shi-yong-Whisper-jin-xing-shi-bie-he-te-zheng-ti-qu.html</link><description>&#13;
&#13;
[Whisper的PT文件下载地址](https://gitcode.csdn.net/65ed73ad1a836825ed799909.html)&#13;
[在Colab微调Whisper](https://huggingface.co/blog/fine-tune-whisper)&#13;
&#13;
## Whisper简介&#13;
&#13;
Whisper是Openai开发的语音识别工具，通常我们可以用Whisper库或者Transformers来使用Whisper，本文专注于Whisper库的使用，安装方式如下&#13;
&#13;
```python&#13;
pip install -U openai-whisper&#13;
```&#13;
&#13;
还需要安装ffmpeg&#13;
```&#13;
conda install ffmpeg(支持非sudo用户)&#13;
sudo apt install ffmpeg &#13;
```&#13;
&#13;
Whisper包含encoder和decoder两个部分,encoder接受30s的音频长度的输出，编码成为特征向量，decoder负责解码&#13;
&#13;
&#13;
## Whisper识别&#13;
`transcribe`:&#13;
- 最简单的识别方式&#13;
```python&#13;
import whisper&#13;
model = whisper.load('path/name')&#13;
text = model.transcribe('wav_path')&#13;
&#13;
```&#13;
`decode`：&#13;
- 注意到音频会被`pad_or_trim`函数填充或者裁剪为30s长度&#13;
- `decode`支持`mel`输入或者`encoder`编码后的特征输入&#13;
- `nmels=80/128`,128适合v3，80适合其他版本&#13;
```python&#13;
import whisper&#13;
import numpy as np&#13;
model = whisper.load_model('')&#13;
audio = whisper.load_audio('')&#13;
audio = whisper.pad_or_trim(audio)&#13;
mel = whisper.log_mel_spectrogram(audio,n_mels=model.dims.n_mels).to('cuda').to(model.device)&#13;
result = model.decode(mel)&#13;
# result = whisper.decode(model, mel)&#13;
&#13;
&#13;
```&#13;
&#13;
```python&#13;
import whisper&#13;
import numpy as np&#13;
model = whisper.load_model('')&#13;
audio = whisper.load_audio('')&#13;
audio = whisper.pad_or_trim(audio)&#13;
mel = whisper.log_mel_spectrogram(audio,n_mels=model.dims.n_mels).to('cuda').to(model.device)&#13;
result =model.decode(mel)&#13;
encoder_output = model.encoder(mel.unsqueeze(0))&#13;
&#13;
result = model.decode(encoder_output)&#13;
# result = whisper.decode(model, encoder_output)&#13;
# 打印encoder输出的形状&#13;
```&#13;
## Whisper提取特征&#13;
如果采用whisper的encoder提取特征，音频首先要被填充到30s&#13;
```python&#13;
import whisper&#13;
import numpy as np&#13;
model = whisper.load_model('')&#13;
audio = whisper.load_audio('')&#13;
audio = whisper.pad_or_trim(audio)&#13;
mel = whisper.log_mel_spectrogram(audio,n_mels=model.dims.n_mels).to('cuda').to(model.device)&#13;
result =model.decode(mel)&#13;
encoder_output = model.encoder(mel.unsqueeze(0))&#13;
&#13;
```&#13;
&#13;
可以采用替代forward函数的方法来提取不定长度的特征,因为encoder不支持小于30s长度音频的原因在于&#13;
- `x = (x + self.positional_embedding).to(x.dtype)`&#13;
&#13;
```python&#13;
import types&#13;
import whisper&#13;
import torch&#13;
import torch.nn as nn&#13;
import torch.nn.functional as F&#13;
def whisper_encoder_forward_monkey_patch(self, x: torch.Tensor):&#13;
	'''&#13;
	x : torch.Tensor, shape = (batch_size, n_mels, n_ctx)&#13;
	the mel spectrogram of the audio&#13;
	'''&#13;
	x = F.gelu(self.conv1(x))&#13;
	x = F.gelu(self.conv2(x))&#13;
	x = x.permute(0, 2, 1)&#13;
	# assert x.shape[1:] == self.positional_embedding.shape, 'incorrect audio shape'&#13;
	# x = (x + self.positional_embedding).to(x.dtype)&#13;
	x = (x + self.positional_embedding[: x.shape[1]]).to(x.dtype)&#13;
	for block in self.blocks:&#13;
		x = block(x)&#13;
		x = self.ln_post(x)&#13;
	return x&#13;
```&#13;
&#13;
```python&#13;
&#13;
encoder = whisper.load_model('base').encoder&#13;
encoder.whisper_encoder_forward_monkey_patch = types.MethodType(whisper_encoder_forward_monkey_patch, encoder)&#13;
audio_path = ''&#13;
audio = whisper.load_audio(audio_path)&#13;
mel = whisper.log_mel_spectrogram(audio).to(model.device)&#13;
features = encoder.whisper_encoder_forward_monkey_patch(mel.unsqueeze(0))&#13;
```&#13;
&#13;
```python&#13;
whisper.model.AudioEncoder.forward = forward&#13;
model = whisper.load_model('')&#13;
audio = whisper.load_audio('')&#13;
mel = whisper.log_mel_spectrogram(audio,n_mels=model.dims.n_mels).to('cuda').to(model.device).unsquenze(0)&#13;
outout = model.encoder(mel)&#13;
```&#13;
如果需要whisper提取出的该特征进行解码，必须使用options&#13;
&#13;
```python&#13;
options = whisper.DecodingOptions(&#13;
    task='transcribe',&#13;
    language='zh',&#13;
    without_timestamps=True,&#13;
    beam_size=4,&#13;
&#13;
)&#13;
print(whisper.decode(model,mel,options))&#13;
```&#13;
&#13;
## Whisper Options&#13;
- `task`:默认为`transcribe`，可以设置为`translate`,即为将输出翻译为英语&#13;
&#13;
&#13;
&#13;
## Huggingface用法&#13;
```python&#13;
from transformers import WhisperForConditionalGeneration, WhisperProcessor&#13;
# 加载预训练的Whisper模型和处理器&#13;
model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-base')&#13;
processor = WhisperProcessor.from_pretrained('openai/whisper-base')&#13;
# 假设你有一个输入的语音特征&#13;
input_features = ...  # 这里应该是预处理后的语音特征&#13;
# 将输入特征移动到模型所在的设备上&#13;
input_features = input_features.to(model.device)&#13;
# 使用分块算法生成输出&#13;
outputs = model.generate(&#13;
    input_features=input_features,&#13;
    return_dict_in_generate=True,&#13;
    output_hidden_states=True,&#13;
    chunk_length=30,  # 设置分块长度&#13;
    stride_length=15  # 设置步长&#13;
)&#13;
# 解码生成的序列&#13;
transcriptions = processor.batch_decode(outputs.sequences, skip_special_tokens=True)[0]&#13;
# 打印转录结果&#13;
print(transcriptions)&#13;
&#13;
```。</description><guid isPermaLink="true">http://teamtee.top/post/yi-wen-gao-dong-Whisper-xi-lie-1--shi-yong-Whisper-jin-xing-shi-bie-he-te-zheng-ti-qu.html</guid><pubDate>Thu, 12 Dec 2024 08:54:05 +0000</pubDate></item></channel></rss>