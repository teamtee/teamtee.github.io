<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>teamtee</title><link>http://teamtee.top</link><description>这里是提姆提的新小屋，旧小屋可以去[这里](http://teamtee.top/teamtee/)</description><copyright>teamtee</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://github.githubassets.com/favicons/favicon.svg</url><title>avatar</title><link>http://teamtee.top</link></image><lastBuildDate>Tue, 24 Dec 2024 02:52:59 +0000</lastBuildDate><managingEditor>teamtee</managingEditor><ttl>60</ttl><webMaster>teamtee</webMaster><item><title>3. nn.Parameters参数</title><link>http://teamtee.top/post/3.%20nn.Parameters-can-shu.html</link><description># 前言&#13;
&#13;
Parameter和Buffer都是实例化的Tensor，Parameter是参与梯度运算的参数，Buffer是不参与梯度计算的参数&#13;
&#13;
`class Parameter(torch.Tensor, metaclass=_ParameterMeta):`&#13;
&#13;
- `Parameter` 是一个特殊的张量，它被用来表示模型的参数,自动将 `Parameter` &#13;
&#13;
 `class Buffer(torch.Tensor, metaclass=_BufferMeta):`&#13;
 &#13;
- `Buffer` 也是一个特殊的张量，它用于存储那些在模型中不直接参与梯度计算的数据，但可能在模型的前向或后向传播中使用。</description><guid isPermaLink="true">http://teamtee.top/post/3.%20nn.Parameters-can-shu.html</guid><pubDate>Tue, 24 Dec 2024 02:52:34 +0000</pubDate></item><item><title>Pytorch复习系列2:Dataset数据集</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-2-Dataset-shu-ju-ji.html</link><description># 前言&#13;
&#13;
Dataset是存储数据的集合，。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-2-Dataset-shu-ju-ji.html</guid><pubDate>Sat, 21 Dec 2024 14:16:18 +0000</pubDate></item><item><title>Pytorch复习系列1:Tensor张量</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-1-Tensor-zhang-liang.html</link><description># 前言&#13;
&#13;
Pytorch计算的基本单位就是Tensor,中文名张量。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-1-Tensor-zhang-liang.html</guid><pubDate>Sat, 21 Dec 2024 11:40:47 +0000</pubDate></item><item><title>Pytorch复习系列0:卷首语</title><link>http://teamtee.top/post/Pytorch-fu-xi-xi-lie-0--juan-shou-yu.html</link><description># 前言&#13;
&#13;
在动笔写下这篇系列的第一篇博客开始，我就必须要提醒自己，为什么要写下《Pytorch入门》的博客，市面上不是有很多笔记和教程了吗，甚至你自己都是通过这些资料来入门Pytorch的，还需要你写入门教程吗？&#13;
&#13;
确实是的，我想市面上的资料已经很全了，但是我觉得还有一些不足：&#13;
- 缺乏系统性：系统性指的是两个方面，知识的结构性和层次性，市面上的资料往往是分散的，缺乏从一个系统的角度来阐明要义，总是局限于某一种应用，并且知识往往不具备层次性，要么过深，要么过浅，要么过度的难度过于陡峭。</description><guid isPermaLink="true">http://teamtee.top/post/Pytorch-fu-xi-xi-lie-0--juan-shou-yu.html</guid><pubDate>Sat, 21 Dec 2024 11:38:45 +0000</pubDate></item><item><title>一文搞懂Whisper系列1:使用Whisper进行识别和特征提取</title><link>http://teamtee.top/post/yi-wen-gao-dong-Whisper-xi-lie-1--shi-yong-Whisper-jin-xing-shi-bie-he-te-zheng-ti-qu.html</link><description>&#13;
&#13;
[Whisper的PT文件下载地址](https://gitcode.csdn.net/65ed73ad1a836825ed799909.html)&#13;
[在Colab微调Whisper](https://huggingface.co/blog/fine-tune-whisper)&#13;
&#13;
## Whisper简介&#13;
&#13;
Whisper是Openai开发的语音识别工具，通常我们可以用Whisper库或者Transformers来使用Whisper，本文专注于Whisper库的使用，安装方式如下&#13;
&#13;
```python&#13;
pip install -U openai-whisper&#13;
```&#13;
&#13;
还需要安装ffmpeg&#13;
```&#13;
conda install ffmpeg(支持非sudo用户)&#13;
sudo apt install ffmpeg &#13;
```&#13;
&#13;
Whisper包含encoder和decoder两个部分,encoder接受30s的音频长度的输出，编码成为特征向量，decoder负责解码&#13;
&#13;
&#13;
## Whisper识别&#13;
`transcribe`:&#13;
- 最简单的识别方式&#13;
```python&#13;
import whisper&#13;
model = whisper.load('path/name')&#13;
text = model.transcribe('wav_path')&#13;
&#13;
```&#13;
`decode`：&#13;
- 注意到音频会被`pad_or_trim`函数填充或者裁剪为30s长度&#13;
- `decode`支持`mel`输入或者`encoder`编码后的特征输入&#13;
- `nmels=80/128`,128适合v3，80适合其他版本&#13;
```python&#13;
import whisper&#13;
import numpy as np&#13;
model = whisper.load_model('')&#13;
audio = whisper.load_audio('')&#13;
audio = whisper.pad_or_trim(audio)&#13;
mel = whisper.log_mel_spectrogram(audio,n_mels=model.dims.n_mels).to('cuda').to(model.device)&#13;
result = model.decode(mel)&#13;
# result = whisper.decode(model, mel)&#13;
&#13;
&#13;
```&#13;
&#13;
```python&#13;
import whisper&#13;
import numpy as np&#13;
model = whisper.load_model('')&#13;
audio = whisper.load_audio('')&#13;
audio = whisper.pad_or_trim(audio)&#13;
mel = whisper.log_mel_spectrogram(audio,n_mels=model.dims.n_mels).to('cuda').to(model.device)&#13;
result =model.decode(mel)&#13;
encoder_output = model.encoder(mel.unsqueeze(0))&#13;
&#13;
result = model.decode(encoder_output)&#13;
# result = whisper.decode(model, encoder_output)&#13;
# 打印encoder输出的形状&#13;
```&#13;
## Whisper提取特征&#13;
如果采用whisper的encoder提取特征，音频首先要被填充到30s&#13;
```python&#13;
import whisper&#13;
import numpy as np&#13;
model = whisper.load_model('')&#13;
audio = whisper.load_audio('')&#13;
audio = whisper.pad_or_trim(audio)&#13;
mel = whisper.log_mel_spectrogram(audio,n_mels=model.dims.n_mels).to('cuda').to(model.device)&#13;
result =model.decode(mel)&#13;
encoder_output = model.encoder(mel.unsqueeze(0))&#13;
&#13;
```&#13;
&#13;
可以采用替代forward函数的方法来提取不定长度的特征,因为encoder不支持小于30s长度音频的原因在于&#13;
- `x = (x + self.positional_embedding).to(x.dtype)`&#13;
&#13;
```python&#13;
import types&#13;
import whisper&#13;
import torch&#13;
import torch.nn as nn&#13;
import torch.nn.functional as F&#13;
def whisper_encoder_forward_monkey_patch(self, x: torch.Tensor):&#13;
	'''&#13;
	x : torch.Tensor, shape = (batch_size, n_mels, n_ctx)&#13;
	the mel spectrogram of the audio&#13;
	'''&#13;
	x = F.gelu(self.conv1(x))&#13;
	x = F.gelu(self.conv2(x))&#13;
	x = x.permute(0, 2, 1)&#13;
	# assert x.shape[1:] == self.positional_embedding.shape, 'incorrect audio shape'&#13;
	# x = (x + self.positional_embedding).to(x.dtype)&#13;
	x = (x + self.positional_embedding[: x.shape[1]]).to(x.dtype)&#13;
	for block in self.blocks:&#13;
		x = block(x)&#13;
		x = self.ln_post(x)&#13;
	return x&#13;
```&#13;
&#13;
```python&#13;
&#13;
encoder = whisper.load_model('base').encoder&#13;
encoder.whisper_encoder_forward_monkey_patch = types.MethodType(whisper_encoder_forward_monkey_patch, encoder)&#13;
audio_path = ''&#13;
audio = whisper.load_audio(audio_path)&#13;
mel = whisper.log_mel_spectrogram(audio).to(model.device)&#13;
features = encoder.whisper_encoder_forward_monkey_patch(mel.unsqueeze(0))&#13;
```&#13;
&#13;
```python&#13;
whisper.model.AudioEncoder.forward = forward&#13;
model = whisper.load_model('')&#13;
audio = whisper.load_audio('')&#13;
mel = whisper.log_mel_spectrogram(audio,n_mels=model.dims.n_mels).to('cuda').to(model.device).unsquenze(0)&#13;
outout = model.encoder(mel)&#13;
```&#13;
如果需要whisper提取出的该特征进行解码，必须使用options&#13;
&#13;
```python&#13;
options = whisper.DecodingOptions(&#13;
    task='transcribe',&#13;
    language='zh',&#13;
    without_timestamps=True,&#13;
    beam_size=4,&#13;
&#13;
)&#13;
print(whisper.decode(model,mel,options))&#13;
```&#13;
&#13;
## Whisper Options&#13;
- `task`:默认为`transcribe`，可以设置为`translate`,即为将输出翻译为英语&#13;
&#13;
&#13;
&#13;
## Huggingface用法&#13;
```python&#13;
from transformers import WhisperForConditionalGeneration, WhisperProcessor&#13;
# 加载预训练的Whisper模型和处理器&#13;
model = WhisperForConditionalGeneration.from_pretrained('openai/whisper-base')&#13;
processor = WhisperProcessor.from_pretrained('openai/whisper-base')&#13;
# 假设你有一个输入的语音特征&#13;
input_features = ...  # 这里应该是预处理后的语音特征&#13;
# 将输入特征移动到模型所在的设备上&#13;
input_features = input_features.to(model.device)&#13;
# 使用分块算法生成输出&#13;
outputs = model.generate(&#13;
    input_features=input_features,&#13;
    return_dict_in_generate=True,&#13;
    output_hidden_states=True,&#13;
    chunk_length=30,  # 设置分块长度&#13;
    stride_length=15  # 设置步长&#13;
)&#13;
# 解码生成的序列&#13;
transcriptions = processor.batch_decode(outputs.sequences, skip_special_tokens=True)[0]&#13;
# 打印转录结果&#13;
print(transcriptions)&#13;
&#13;
```。</description><guid isPermaLink="true">http://teamtee.top/post/yi-wen-gao-dong-Whisper-xi-lie-1--shi-yong-Whisper-jin-xing-shi-bie-he-te-zheng-ti-qu.html</guid><pubDate>Thu, 12 Dec 2024 08:54:05 +0000</pubDate></item></channel></rss>